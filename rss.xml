<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>B.log - RSS feed</title>
        <link>http://barmaley.exe.name</link>
        <description><![CDATA[Random notes on Software Engineering, Computer Science and Mathematics]]></description>
        <atom:link href="http://barmaley.exe.name/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Tue, 05 Jul 2016 00:00:00 UT</lastBuildDate>
        <item>
    <title>Neural Variational Inference: Blackbox Mode</title>
    <link>http://barmaley.exe.name/posts/2016-07-05-neural-variational-inference-blackbox.html</link>
    <description><![CDATA[<p>In the previous post we covered Stochastic VI: an efficient and scalable variational inference method for exponential family models. However, there’re many more distributions than those belonging to the exponential family. Inference in these cases requires significant amount of model analysis. In this post we consider <a href="https://arxiv.org/abs/1401.0118">Black Box Variational Inference</a> by Ranganath et al. This work just as the previous one comes from <a href="http://www.cs.columbia.edu/~blei/">David Blei</a> lab — one of the leading researchers in VI. And, just for the dessert, we’ll touch upon another paper, which will finally introduce some neural networks in VI.</p>
<!--more-->

<h3>
Blackbox Variational Inference
</h3>

<p>As we have learned so far, the goal of VI is to maximize the ELBO <span class="math">\(\mathcal{L}(\Theta, \Lambda)\)</span>. When we maximize it by <span class="math">\(\Lambda\)</span>, we decrease the gap between the marginal likelihood of the model considered <span class="math">\(\log p(x \mid \Theta)\)</span>, and when we maximize it by <span class="math">\(\Theta\)</span> we acltually fit the model. So let’s concentrate on optimizing this objective:</p>
<p><span class="math">\[
\mathcal{L}(\Theta, \Lambda) = \mathbb{E}_{q(z \mid x, \Lambda)} \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right]
\]</span></p>
<p>Let’s find gradients of this objective:</p>
<p><span class="math">\[
\begin{align}
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
&amp;= \nabla_{\Lambda} \int q(z \mid x, \Lambda) \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right] dz  \\
&amp;= \int \nabla_{\Lambda} q(z \mid x, \Lambda) \left[\log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right] dz - \int q(z \mid x, \Lambda) \nabla_{\Lambda} \log q(z \mid x, \Lambda) dz  \\
&amp;= \mathbb{E}_{q} \left[\frac{\nabla_{\Lambda} q(z \mid x, \Lambda)}{q(z \mid x, \Lambda)} \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \int q(z \mid x, \Lambda) \frac{\nabla_{\Lambda} q(z \mid x, \Lambda)}{q(z \mid x, \Lambda)} dz \\ 
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \int \nabla_{\Lambda} q(z \mid x, \Lambda) dz \\ 
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right] - \nabla_{\Lambda} \overbrace{\int q(z \mid x, \Lambda) dz}^{=1} \\
&amp;= \mathbb{E}_{q} \left[\nabla_{\Lambda} \log q(z \mid x, \Lambda) \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)} \right]
\end{align}
\]</span></p>
<p>In statistics <span class="math">\(\nabla_\Lambda \log q(z \mid x, \Lambda)\)</span> is known as <a href="https://en.wikipedia.org/wiki/Score_(statistics)">score function</a>. For more on this “trick” see <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">a blogpost by Shakir Mohamed</a>. In many cases of practical interest <span class="math">\(\log p(x, z, \mid \Theta)\)</span> is too complicated to compute this expectation in closed form. Recall that we already used stochastic optimization successfully, so we can settle with just an estimate of true gradient. We get one by approximating the expectation using Monte-Carlo estimates using <span class="math">\(L\)</span> samples <span class="math">\(z^{(l)} \sim q(z \mid x, \Lambda)\)</span>:</p>
<p><span class="math">\[
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)}
\]</span></p>
<p>For model parameters <span class="math">\(\Theta\)</span> gradients look even simpler, as we don’t need to differentiate w.r.t. expectation distribution’s parameters:</p>
<p><span class="math">\[
\begin{align}
\nabla_{\Theta} \mathcal{L}(\Theta, \Lambda)
&amp;= \mathbb{E}_{q} \nabla_{\Theta} \log p(x, z \mid \Theta)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Theta} \log p(x, z^{(l)} \mid \Theta)
\end{align}
\]</span></p>
<p>We can even “naturalize” these gradients by premultiplying by the inverse Fisher Information Matrix <span class="math">\(\mathcal{I}(\Lambda)^{-1}\)</span>. And that’s it! Much simpler than before, right? Of course, there’s no free lunch, so there must be a catch… And there is: performance of stochastic optimization methods crucially depends on the variance of gradient estimators. It makes perfect sense: the higher the variance — the less information about the step direction we get. And unfortunately, in practice the aforementioned estimator based on the score function has impractically high variance. Luckily, in Monte Carlo community there are many variance reductions techniques known, we now describe some of them.</p>
<p>The first technique we’ll describe is <strong>Rao-Blackwellization</strong>. The idea is simple: if it’s possible to compute the expectation w.r.t. some of random variables, you should do it. If you think of it, it’s an obvious advice as you essentially reduce amount of randomness in your Monte Carlo estimates. But let’s put it more formally: we use chain rule to rewrite joint expectation as marginal expectation of conditional one:</p>
<p><span class="math">\[
\mathbb{E}_{X, Y} f(X, Y) = \mathbb{E}_X \left[ \mathbb{E}_{Y \mid X} f(X, Y) \right]
\]</span></p>
<p>Let’s see what happens with variance (in scalar case) when we estimate expectation of <span class="math">\(\mathbb{E}_{Y \mid X} f(X, Y)\)</span> instead of expectation of <span class="math">\(f(X, Y)\)</span>:</p>
<p><span class="math">\[
\begin{align}
\text{Var}_X(\mathbb{E}_{Y \mid X} f(X, Y))
&amp;= \mathbb{E} (\mathbb{E}_{Y \mid X} f(X, Y))^2 - (\mathbb{E}_{X, Y} f(X, Y))^2 \\
&amp;= \text{Var}_{X,Y}(f(X, Y)) - \mathbb{E}_X \left(\mathbb{E}_{Y \mid X} f(X, Y)^2 - (\mathbb{E}_{Y \mid X} f(X, Y))^2  \right) \\
&amp;= \text{Var}_{X,Y}(f(X, Y)) - \mathbb{E}_X \text{Var}_{Y\mid X} (f(X, Y))
\end{align}
\]</span></p>
<p>This formula says that Rao-Blackwellizing an estimator reduces its variance by <span class="math">\(\mathbb{E}_X \text{Var}_{Y\mid X} (f(X, Y))\)</span>. Indeed, you can think of this term as of a measure of how much information <span class="math">\(Y\)</span> contains about <span class="math">\(X\)</span> that’s relevant to computing <span class="math">\(f(X, Y)\)</span>. Suppose <span class="math">\(Y = X\)</span>: then you have <span class="math">\(\mathbb{E}_X f(X, X)\)</span>, and taking expectation w.r.t. <span class="math">\(Y\)</span> does not reduce amount of randomness in the estimator. And this is what the formula tells us as <span class="math">\(\text{Var}_{Y \mid X} f(X, Y)\)</span> would be 0 in this case. Here’s another example: suppose <span class="math">\(f\)</span> does not use <span class="math">\(X\)</span> at all: then only randomness in <span class="math">\(Y\)</span> affects the estimate, and after Rao-Blackwellization we expect the variance to drop to 0. And the formula agrees with out expectations as <span class="math">\(\mathbb{E}_X \text{Var}_{Y \mid X} f(X, Y) = \text{Var}_Y f(X, Y)\)</span> for any <span class="math">\(X\)</span> since <span class="math">\(f(X, Y)\)</span> does not depend on <span class="math">\(X\)</span>.</p>
<p>Next technique is <strong>Control Variates</strong>, which is slightly less intuitive. The idea is that we can add zero-mean function <span class="math">\(h(X)\)</span> that’ll preserve the expectation, but reduce the variance. Again, for a scalar case</p>
<p><span class="math">\[
\text{Var}(f(X) - \alpha h(X)) = \text{Var}(f(X)) - 2 \alpha \text{Cov}(f(X), h(X)) + \alpha^2 \text{Var}(f(X))
\]</span></p>
<p>Optimal <span class="math">\(\alpha^* = \frac{\text{Cov}(f(X), h(X))}{\text{Var}(f(X))}\)</span>. This formula reflects an obvious fact: if we want to reduce the variance, <span class="math">\(h(X)\)</span> must be correlated with <span class="math">\(f(X)\)</span>. Sign of correlation does not matter, as <span class="math">\(\alpha^*\)</span> will adjust. BTW, in reinforcement learning <span class="math">\(\alpha\)</span> is called <strong>baseline</strong>.</p>
<p>As we already have learned, <span class="math">\(\mathbb{E}_{q(z \mid x, \Lambda)} \nabla_\Lambda \log q(z \mid x, \Lambda) = 0\)</span>, so the score function is a good candidate for <span class="math">\(h(x)\)</span>. Therefore our estimates become</p>
<p><span class="math">\[
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \circ \left(\log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* \right)
\]</span></p>
<p>Where <span class="math">\(\circ\)</span> is pointwise multiplication and <span class="math">\(\alpha\)</span> is a vector of <span class="math">\(|\Lambda|\)</span> components with <span class="math">\(\alpha_i\)</span> being a baseline for variational parameter <span class="math">\(\Lambda_i\)</span>:</p>
<p><span class="math">\[
\alpha^*_i = \frac{\text{Cov}(\nabla_{\Lambda_i} \log q(z \mid x, \Lambda)\left( \log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right), \nabla_{\Lambda_i} \log q(z \mid x, \Lambda))}{\text{Var}(\nabla_{\Lambda_i} \log q(z \mid x, \Lambda)\left( \log p(x, z \mid \Theta) - \log q(z \mid x, \Lambda) \right))}
\]</span></p>
<h3>
Neural Variational Inference
</h3>

<p>Hoooray, neural networks! In this section I’ll briefly describe a variance reduction technique coined by A. Mnih and K. Gregor in <a href="https://arxiv.org/abs/1402.0030">Neural Variational Inference and Learning in Belief Networks</a>. The idea is surprisingly simple: why not learn the baseline <span class="math">\(\alpha\)</span> using a neural network?</p>
<p><span class="math">\[
\nabla_{\Lambda} \mathcal{L}(\Theta, \Lambda)
\approx \frac{1}{L} \sum_{l=1}^L \nabla_{\Lambda} \log q(z^{(l)} \mid x, \Lambda) \circ \left(\log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* - \alpha(x) \right)
\]</span></p>
<p>Where <span class="math">\(\alpha(x)\)</span> is a neural network trained to minimize</p>
<p><span class="math">\[
 \mathbb{E}_{q(z \mid x, \Lambda)} \left( \log \frac{p(x, z^{(l)} \mid \Theta)}{q(z^{(l)} \mid x, \Lambda)} - \alpha^* - \alpha(x) \right)^2
\]</span></p>]]></description>
    <pubDate>Tue, 05 Jul 2016 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2016-07-05-neural-variational-inference-blackbox.html</guid>
</item>
<item>
    <title>Neural Variational Inference: Scaling Up</title>
    <link>http://barmaley.exe.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html</link>
    <description><![CDATA[<p>In the previous post I covered well-established classical theory developed in early 2000-s. Since then technology has made huge progress: now we have much more data, and a great need to process it and process it fast. In big data era we have huge datasets, and can not afford too many full passes over it, which might render classical VI methods impractical. Recently M. Hoffman et al. dissected classical Mean-Field VI to introduce stochasticity right into its heart, which resulted in <a href="https://arxiv.org/abs/1206.7051">Stochastic Variational Inference</a>.</p>
<!--more-->

<h3>
Stochastic Variational Inference
</h3>

<p>We start with model assumptions: we have 2 types of latent variables, the global latent variable <span class="math">\(\beta\)</span> and a bunch of local variables <span class="math">\(z_n\)</span> for each observation <span class="math">\(x_n\)</span>. Recalling our GMM example, <span class="math">\(\beta\)</span> can be thought of as a mixture weights <span class="math">\(\pi\)</span>, and <span class="math">\(z_n\)</span> are membership indicators, as previously. These variables are assumed to come from some exponential family distribution:</p>
<p><span class="math">\[
p(x_n, z_n \mid \beta) = h(x_n, z_n) \exp \left( \beta^T t(x_n, z_n) - a_l(\beta) \right) \\
\\
p(\beta) = h(\beta) \exp(\alpha^T t(\beta) - a_g(\alpha))
\]</span></p>
<p>Where <span class="math">\(t(\cdot)\)</span> and <span class="math">\(h(\cdot)\)</span> are overloaded by their argument, so <span class="math">\(t(\beta)\)</span> and <span class="math">\(t(z_{nj})\)</span> correspond to two different functions. <span class="math">\(t(\cdot)\)</span> gives a <strong>natural parameter</strong> and also <strong>sufficient statistics</strong>. <span class="math">\(a_g\)</span> and <span class="math">\(a_l\)</span> are log-normalizing constants which for exponential family distributions have an interesting property, namely, the gradient of the log-normalizing constant is the expectation of sufficient statistics: <span class="math">\(\nabla_\alpha a_g(\alpha) = \mathbb{E} t(\beta)\)</span>.</p>
<p>From these assumptions we can derive <em>complete conditionals</em> (conditional distribution given all other hidden variables) for <span class="math">\(\beta\)</span> and <span class="math">\(z_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
p(\beta \mid x, z, \alpha)
&amp;\propto \prod_{n=1}^N p(x_n, z_n \mid \beta) p(\beta \mid \alpha) \\
&amp;= h(\beta) \prod_{n=1}^N h(x_n, z_n) \exp \left( \beta^T \sum_{n=1}^N t(x_n, z_n) - N a_l(\beta) + \alpha^T t(\beta) - a_g(\alpha) \right) \\
&amp;\propto h(\beta) \exp \left( \eta_g(x, z, \alpha)^T t(\beta) \right)
\end{align}
\]</span></p>
<p>Where <span class="math">\(t(\beta) = (\beta, -a_l(\beta))\)</span>, <span class="math">\(\eta_g(x, z, \alpha) = (\alpha_1 + \sum_{n=1}^N t(x_n, z_n), \alpha_2 + N)\)</span>. We see that the (unnormalized) posterior distribution for <span class="math">\(\beta\)</span> has the same functional form as the (unnormalized) prior <span class="math">\(p(\beta)\)</span>, therefore after normalization it’d be</p>
<p><span class="math">\[
p(\beta \mid x, z, \alpha)
= h(\beta) \exp \left( \eta_g(x, z, \alpha)^T t(\beta) - a_g(\eta_g(x, z, \alpha)) \right)
\]</span></p>
<p>The same applies to local variables <span class="math">\(z_{nj}\)</span>:</p>
<p><span class="math">\[
p(z_{nj} \mid x_n, z_{n,-j}, \beta)
\propto h(z_{nj}) \exp \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) \right)
\]</span> Hence <span class="math">\[
p(z_{nj} \mid x_n, z_{n,-j}, \beta)
= h(z_{nj}) \exp \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) - a_m(\eta_l(x_n, z_{n,-j}, \beta)) \right)
\]</span></p>
<p>Even though we’ve managed to find the complete conditional for <span class="math">\(\beta\)</span>, it might be intractable to find the posterior for all latent variables <span class="math">\(p(\beta, z \mid x, \alpha)\)</span>. We therefore turn to the mean field approximation:</p>
<p><span class="math">\[
q(z, \beta \mid \Lambda) = q(\beta \mid \lambda) \prod_{n=1}^N \prod_{j=1}^J q(z_{nj} \mid \phi_{nj})
\]</span></p>
<p>We assume these marginal distributions come from the exponential family:</p>
<p><span class="math">\[
q(\beta \mid \lambda) = h(\beta) \exp(\lambda^T t(\beta) - a_g(\lambda)) \\
q(z_{nj} \mid \phi_{nj}) = h(z_{nj}) \exp(\phi_{nj}^T t(z_{nj}) - a_m(\phi_{nj}))
\]</span></p>
<p>Let’s find the optimal variational parameters now by optimizing the ELBO <span class="math">\(\mathcal{L}(\Theta, \Lambda)\)</span> (<span class="math">\(\Theta\)</span> is model parameters, <span class="math">\(\alpha\)</span>, and <span class="math">\(\Lambda\)</span> contains variational parameters <span class="math">\(\phi\)</span> and <span class="math">\(\lambda\)</span>) by <span class="math">\(\lambda\)</span> and <span class="math">\(\phi_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\lambda)
&amp;= \mathbb{E}_{q} \left( \log p(x, z, \beta) - \log q(\beta) - \log q(z) \right)
= \mathbb{E}_{q} \left( \log p(\beta \mid x, z) - \log q(\beta) \right) + \text{const} \\
&amp;= \mathbb{E}_{q} \left( \eta_g(x, z, \alpha)^T t(\beta) - \lambda^T t(\beta) + a_g(\lambda) \right) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)^T \mathbb{E}_{q(\beta)} t(\beta) + a_g(\lambda) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)^T \nabla_\lambda a_g(\lambda) t(\beta) + a_g(\lambda) + \text{const}
\end{align}
\]</span></p>
<p>Where we used aforementioned property of exponential family distributions: <span class="math">\(\nabla_\lambda a_g(\lambda) = \mathbb{E}_{q(\beta)} t(\beta)\)</span>. The gradient then is <span class="math">\[
\nabla_\lambda \mathcal{L}(\lambda)
= \nabla_\lambda^2 a_g(\lambda) \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)
\]</span></p>
<p>After setting it to zero we get an update for global latent variables: <span class="math">\(\lambda = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha)\)</span>. Following the same reasoning we derive the optimal update for <span class="math">\(\phi_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\phi_{nj})
&amp;= \mathbb{E}_{q} \left( \log p(z_{nj} \mid x_n, z_{n,-j}, \beta) - \log q(z_{nj}) \right) + \text{const} \\
&amp;= \mathbb{E}_{q} \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) - \phi_{nj}^T t(z_{nj}) + a_m(\phi_{nj})\right) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} \right)^T \mathbb{E}_{q(z_{nj})} t(z_{nj}) + a_m(\phi_{nj}) + \text{const} \\
\end{align}
\]</span></p>
<p>The gradient then is <span class="math">\(\nabla_{\phi_{nj}} \mathcal{L}(\phi) = \nabla_{\phi_{nj}}^2 a_m(\phi_{nj}) \left(\mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} \right)\)</span>, and the update is <span class="math">\(\phi_{nj} = \mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta)\)</span>.</p>
<p>So far we found mean-field updates, as well as corresponding gradients of the ELBO for variational parameters <span class="math">\(\lambda\)</span> and <span class="math">\(\phi_{nj}\)</span>. Next step is to transform these gradients into <strong>natural gradients</strong>. Intuitively, classical gradient defines local linear approximation, where the notion of locality comes from the Euclidean space. However, parameters influence the ELBO only through distributions <span class="math">\(q\)</span>, so we might like to alter our idea of locality based on how much the distributions change. This is what natural gradient does: it defines local linear approximation where locality means small distance (symmetrized KL-divergence) between distributions. There’s great formal explanation in the paper, and if you want to read more on that matter, I refer you to a great post by Roger Grosse, <a href="http://www.metacademy.org/roadmaps/rgrosse/dgml">Differential geometry for machine learning</a>.</p>
<p>The natural gradient can be obtained from the usual gradient using a simple linear transformation:</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} f(\lambda) = \mathcal{I}(\lambda)^{-1} \nabla_{\lambda} f(\lambda)
\]</span></p>
<p>Where <span class="math">\(\mathcal{I}(\lambda) := \mathbb{E}_{q(\beta \mid \lambda)} \left[ \nabla_\lambda \log q(\beta \mid \lambda) (\nabla_\lambda \log q(\beta \mid \lambda))^T \right]\)</span> is Fisher Information Matrix. Here I considered parameter <span class="math">\(\lambda\)</span> of the distribution <span class="math">\(q(\beta \mid \lambda)\)</span>, you got the idea. For the exponential family distribution this Information Matrix takes an especially simple form:</p>
<p><span class="math">\[
\begin{align}
\mathcal{I}(\lambda)
&amp;= \mathbb{E}_q (t(\beta) - \nabla_\lambda a_g(\lambda)) (t(\beta) - \nabla_\lambda a_g(\lambda))^T
= \mathbb{E}_q (t(\beta) - \mathbb{E}_q t(\beta)) (t(\beta) - \mathbb{E}_q t(\beta))^T \\
&amp;= \text{Cov}_q (t(\beta))
 = \nabla_\lambda^2 a_g(\lambda) 
\end{align}
\]</span></p>
<p>Where we’ve used another <a href="https://en.wikipedia.org/wiki/Exponential_family#Differential_identities_for_cumulants">differential identity for exponential family</a>. All these calculations lead us to the natural gradients of ELBO for variational parameters:</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} \mathcal{L}(\lambda) = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \\
\nabla_{\phi_{nj}}^\text{N} \mathcal{L}(\lambda) = \mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} 
\]</span></p>
<p>Surprisingly, computation-wise calculating natural gradients is even simpler that calculating classical gradients! There’s an interesting connection between the mean-field update and a natural gradient step. In particular, if we make a step along the natural gradient with step size equal 1, we’d get <span class="math">\(\lambda^{\text{new}} = \lambda^{\text{old}} + (\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda^{\text{old}}) = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha)\)</span>. The same applies to parameters <span class="math">\(\phi\)</span>. This means that the mean field updates are exactly natural gradient steps, and vice versa.</p>
<p>Recall, we have derived mean field updates by finding a minima of KL-divergence with the true posterior, that is in just one step (one update) we arrive at minimum. Obviously, we have the same in the natural gradient formulation, when just one step brings us to the optimum.</p>
<p>Now, the last component is stochasticity itself. So far we have only played a little with mean-field update scheme, and discovered its connection to the natural gradient optimization. We note that we have 2 parameters: local <span class="math">\(\phi_{nj}\)</span> and global parameter <span class="math">\(\lambda\)</span>. The first one is easy to optimize over as it depends only on one, <span class="math">\(n\)</span>th sample <span class="math">\(x_n\)</span>. The second one, though, needs to incorporate information from all the samples, which is computationally prohibitive in large scale regime. Luckily, now once we know the equivalence between mean-field update and natural gradient step, we can borrow ideas from stochastic optimization to make this process more scalable.</p>
<p>Let’s first reformulate the ELBO to include the sum over samples <span class="math">\(x_n\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\Theta, \Lambda)
&amp;= \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \sum_{n=1}^N \left(\log p(x_n, z_n \mid \beta) - \log q(z_n \mid \phi_n) \right) \right] \\
&amp; = \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + N \mathbb{E}_{I} \left(\log p(x_I, z_I \mid \beta) - \log q(z_I \mid \phi_I) \right) \right]
\end{align}
\]</span></p>
<p>Where <span class="math">\(I \sim \text{Unif}\{1, \dots, N\}\)</span> — uniformly distribution index of a sample. Now let’s estimate <span class="math">\(\mathcal{L}\)</span> using a sample <span class="math">\(S\)</span> (assume <span class="math">\(N\)</span> divides by sample size <span class="math">\(|S|\)</span>) of uniformly chosen indices, this’d result in an unbiased estimator (it’s gradient would also be unbiased, so we can maximize the true ELBO by maximizing the estimate). Author of the paper start with single-sample derivation and then extend it to minibatches, but I decided I’d go straight to the minibatch case:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}_S(\Theta, \Lambda)
&amp; := \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \frac{N}{|S|} \sum_{i \in S} \left(\log p(x_i, z_i \mid \beta) - \log q(z_i \mid \phi_i) \right) \right] \\
&amp; = \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \sum_{n=1}^{N / |S|} \sum_{i \in S} \left(\log p(x_i, z_i \mid \beta) - \log q(z_i \mid \phi_i) \right) \right]
\end{align}
\]</span></p>
<p>This estimate is exactly <span class="math">\(\mathcal{L}(\Theta, \Lambda)\)</span> calculated on sample consisting of <span class="math">\(\{x_i, z_i\}_{i \in S}\)</span> repeated <span class="math">\(N / |S|\)</span> times. Hence its natural gradient w.r.t. <span class="math">\(\lambda\)</span> is</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} \mathcal{L}_S(\lambda) = \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha) - \lambda \\
\]</span></p>
<p>One important note: for stochastic optimization we can’t use constant step size. As Robbins-Monro conditions suggest, we need to use schedule <span class="math">\(\rho_t\)</span> such that <span class="math">\(\sum \rho_t = \infty\)</span> and <span class="math">\(\sum \rho_t^2 &lt; \infty\)</span>. Then the update <span class="math">\(\lambda^{\text{new}} = \lambda^{\text{old}} + \rho_t \nabla_\lambda^\text{N} \mathcal{L}_S(\lambda) = (1 - \rho_t) \lambda^{\text{old}} + \rho_t \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha)\)</span></p>
<p>Finally we have the following optimization scheme:</p>
<ul>
	<li>
Start with random initialization for <span class="math">\(\lambda^{(0)}\)</span>
</li>
	<li>
For <span class="math">\(t\)</span> from 0 to MAX_ITER
<ol>
		<li>
Sample <span class="math">\(S \sim \text{Unif}\{1, \dots, N\}^{|S|}\)</span>
</li>
		<li>
For each sample <span class="math">\(i \in S\)</span> update the local variational parameter <span class="math">\(\phi_{i,j} = \mathbb{E}_{q(\beta) q(z_{i,-j})} \eta_l(x_i, z_{i,-j}, \beta)\)</span>
</li>
		<li>
Replicate the sample <span class="math">\(N / |S|\)</span> times and compute the global update <span class="math">\(\hat \lambda = \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha)\)</span>
</li>
		<li>
Update the global update <span class="math">\(\lambda^{(t+1)} = (1-\rho_t) \lambda^{(t)} + \rho_t \hat \lambda\)</span>
</li>
	</ol>
	</li>
</ul>

]]></description>
    <pubDate>Mon, 04 Jul 2016 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html</guid>
</item>
<item>
    <title>Neural Variational Inference: Classical Theory</title>
    <link>http://barmaley.exe.name/posts/2016-07-01-neural-variational-inference-classical-theory.html</link>
    <description><![CDATA[<p>As a member of <a href="http://bayesgroup.ru/">Bayesian methods research group</a> I’m heavily interested in Bayesian approach to machine learning. One of the strengths of this approach is ability to work with hidden (unobserved) variables which are interpretable. This power however comes at a cost of generally intractable exact inference, which limits the scope of solvable problems.</p>
<p>Another topic which gained lots of momentum in Machine Learning recently is Deep Learning, of course. With Deep Learning we can now build big and complex models that outperform most hand-engineered approaches given lots of data and computational power. The fact that Deep Learning needs a considerable amount of data also requires these methods to be scalable — a really nice property for any algorithm to have, especially in a Big Data epoch.</p>
<p>Given how appealing both topic are it’s not a surprise there’s been some work to marry these two recently. In this series of blogsposts I’d like to summarize recent advances, particularly in variational inference. This is not meant to be an introductory discussion as prior familiarity with classical topics (Latent variable models, <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference, Mean-field approximation</a>) is required, though I’ll introduce these ideas anyway just to remind it and setup the notation.</p>
<!--more-->

<h3>
Latent Variables Models
</h3>

<p>Suppose you have a probabilistic model that’s easy to describe using some auxiliary variables <span class="math">\(Z\)</span> that you don’t observe directly (or even would like to infer it given the data). One classical example for this setup is Gaussian Mixture Modeling: we have <span class="math">\(K\)</span> components in a mixture, and <span class="math">\(z_n\)</span> is a <a href="https://en.wikipedia.org/wiki/One-hot">one-hot</a> vector of dimensionality <span class="math">\(K\)</span> indicating which component an observation <span class="math">\(x_n\)</span> belongs to. Then, conditioned on <span class="math">\(z_n\)</span> the distribution of <span class="math">\(x_n\)</span> is a usual Gaussian distribution: <span class="math">\(p(x_{n} \mid z_{nk} = 1) = \mathcal{N}(x_n \mid \mu_k, \Sigma_k)\)</span> (here whenever I refer to a distribution, you should read it as its density. At least <a href="https://en.wikipedia.org/wiki/Generalized_function">generalized one</a>). Therefore the joint distribution of the model is</p>
<p><span class="math">\[
p(x, z \mid \Theta) = \prod_{n=1}^N \prod_{k=1}^K \mathcal{N}(x_n \mid \mu_k, \Sigma_k)^{z_{nk}} \pi_k^{z_{nk}}
\]</span></p>
<p>Where <span class="math">\(\pi\)</span> is a probability distribution over <span class="math">\(K\)</span> outcomes, and <span class="math">\(\Theta\)</span> is a set of all model’s parameters (<span class="math">\(\pi\)</span>, <span class="math">\(\mu\)</span>s and <span class="math">\(\Sigma\)</span>s).</p>
<p>We’d like to do 2 things with the model: first, we obviously need to learn parameters <span class="math">\(\Theta\)</span>, and second, we’d like infer these latent variables <span class="math">\(z_n\)</span> to know which cluster the observation <span class="math">\(x_n\)</span> belongs to, that is, we need to calculate the distribution of <span class="math">\(z_n\)</span> conditioned on <span class="math">\(x_n\)</span>: <span class="math">\(p(z_n \mid x_n)\)</span>.</p>
<p>We want to learn the parameters <span class="math">\(\Theta\)</span> as usual by maximizing the log-likelihood. Unfortunately, we don’t know true assignments <span class="math">\(z_n\)</span>, and marginalizing over it as in <span class="math">\(p(x_n) = \sum_{k=1}^K \pi_k p(x_n, z_{nk} = 1)\)</span> is not a good idea as the resulting optimization problem would lose its convexity. Instead we decompose the log-likelihood as follows:</p>
<p><span class="math">\[
\begin{align}
\log p(x)
&amp;= \mathbb{E}_{q(z\mid x)} \overbrace{\log p(x)}^{\text{const in $z$}}
= \mathbb{E}_{q(z\mid x)} \log \frac{p(x, z) q(z\mid x)}{p(z \mid x) q(z\mid x)}  \\
&amp;= \mathbb{E}_{q(z\mid x)} \log \frac{p(x, z)}{q(z\mid x)} + D_{KL}(q(z\mid x) \mid\mid p(z \mid x))
\end{align}
\]</span></p>
<p>The second term is a Kullback-Leibler divergence, which is always non-negative, and equals zero iff distributions are equal almost everywhere <span class="math">\(q(z\mid x) = p(z \mid x)\)</span>. Therefore putting <span class="math">\(q(z \mid x) = p(z \mid x)\)</span> eliminates the second term, leaving us with <span class="math">\(\log p(x) = \mathbb{E}_{p(z \mid x)} \log \frac{p(x, z)}{p(z \mid x)}\)</span>. Therefore all we need to be able to do is to calculate the posterior <span class="math">\(p(z \mid x)\)</span>, and maximize the expectation. This is how EM algorithm is derived: at E-step we calculate the posterior <span class="math">\(p(z \mid x, \Theta^{\text{old}})\)</span>, and at M-step we maximize the expectation <span class="math">\(\mathbb{E}_{p(z \mid x, \Theta^{\text{old}})} \log \frac{p(x, z \mid \Theta)}{p(z \mid x, \Theta)}\)</span> with respect to <span class="math">\(\Theta\)</span> keeping <span class="math">\(\Theta^{\text{old}}\)</span> fixed.</p>
<p>Now, all we are left to do is to find the posterior <span class="math">\(p(z \mid x)\)</span> which is given by the following deceivingly simple formula knows as a Bayes’ rule.</p>
<p><span class="math">\[
p(z \mid x) = \frac{p(x \mid z) p(z)}{\int p(x \mid z) p(z)dz}
\]</span></p>
<p>Of course, there’s no free lunch and computing the denominator is intractable in general case. One <strong>can</strong> compute the posterior exactly when the prior <span class="math">\(p(z)\)</span> and the likelihood <span class="math">\(p(x \mod z)\)</span> are conjugate, but many models of practical interest don’t have this property. This is where variational inference comes in.</p>
<h3>
Variational Inference and Mean-field
</h3>
<p>In a variational inference (VI) framework we approximate the true posterior <span class="math">\(p(z \mid x)\)</span> with some other simpler distribution <span class="math">\(q(z \mid x, \Lambda)\)</span> where <span class="math">\(\Lambda\)</span> is a set of (variational) parameters of the (variational) approximation (I may omit <span class="math">\(\Lambda\)</span> and <span class="math">\(\Theta\)</span> in a “given” clause when it’s convenient, remember, it always could be placed there). One possibility is to divide latent variables <span class="math">\(z\)</span> in groups and force them to be independent. In extreme case each variable gets its own group, assuming independence among all variables <span class="math">\(q(z \mid x) = \prod_{d=1}^D q(z_d \mid x)\)</span>. If we then set about to find the best approximation to the true posterior in this fully factorized class, we will no longer have the optimal <span class="math">\(q\)</span> being the true posterior itself, as the true posterior is presumably too complicated to be dealt with in analytic form (which we want from the approximation <span class="math">\(q\)</span> when we say “simpler distribution”). Therefore we find the optimal <span class="math">\(q(z_i)\)</span> by minimizing the KL-divergence with the true posterior:</p>
<p><span class="math">\[
\begin{align}
D_{KL}(q(z \mid x) \mid\mid p(z \mid x))
&amp;= \mathbb{E}_{q(z_i \mid x)} \left[ \mathbb{E}_{q(z_{-i} \mid x)} \log \frac{q(z_1 \mid x) \dots q(z_D \mid x)}{p(z \mid x)} \right] \\
&amp;= \mathbb{E}_{q(z_i \mid x)} \left[ \log q(z_i \mid x) - \underbrace{\mathbb{E}_{q(z_{-i} \mid x)} \log p(z \mid x)}_{\log f(z_i \mid x)} \right] + \text{const} \\
&amp;= \mathbb{E}_{q(z_i \mid x)} \left[ \log \frac{q(z_i \mid x)}{\tfrac{1}{Z} f(z_i \mid x)} \right] - \log Z + \text{const} \\
&amp;= D_{KL}\left(q(z_i \mid x) \mid\mid \tfrac{1}{Z} f(z_i \mid x)\right) + \text{const}
\end{align}
\]</span></p>
<p>For many models it’s possible to look into <span class="math">\(\mathbb{E}_{q(z_{-i} \mid x)} \log p(z \mid x)\)</span> and immediately recognize logarithm of unnormalized density of some known distribution.</p>
<p>Another cornerstone of this framework is a notion of <strong>Evidence Lower Bound</strong> (ELBO): recall the decomposition of log-likelihood we derived above. In our current setting we can not compute the right hand side as we can not evaluate the true posterior <span class="math">\(p(z \mid x)\)</span>. However, note that the left hand side (that is, the log-likelihood) does not depend on the variational distribution <span class="math">\(q(z \mid x, \Lambda)\)</span>. Therefore, maximizing the first term of the right hand side w.r.t. variational parameters <span class="math">\(\Lambda\)</span> results in minimizing the second term, the KL-divergence with the true posterior. This implies we can ditch the second term, and maximize the first one w.r.t. both model parameters <span class="math">\(\Theta\)</span> and variational parameters <span class="math">\(\Lambda\)</span>:</p>
<p><span class="math">\[
\text{ELBO:} \quad \mathcal{L}(\Theta, \Lambda) := \mathbb{E}_{q(z \mid x, \Lambda)} \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)}
\]</span></p>
<p>Okay, so this covers the basics, but before we go to the neural networks-based methods we need to discuss some general approaches to VI and how to make it scalable. This is what the next blog post is all about.</p>]]></description>
    <pubDate>Fri, 01 Jul 2016 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2016-07-01-neural-variational-inference-classical-theory.html</guid>
</item>
<item>
    <title>Exploiting Multiple Machines for Embarrassingly Parallel Applications</title>
    <link>http://barmaley.exe.name/posts/2014-08-01-gnu-parallel.html</link>
    <description><![CDATA[<p>During work on my machine learning project I was needed to perform some quite computation-heavy calculations several times — each time with a bit different inputs. These calculations were CPU and memory bound, so just spawning them all at once would just slow down overall running time because of increased amount of context switches. Yet running 4 (=number of cores in my CPU) of them at a time (actually, 3 since other applications need CPU, too) should speed it up.</p>
<p>Fortunately, I have an old laptop with 2 cores as well as an access to somewhat more modern machine with 4 cores. That results in 10 cores spread across 3 machines (all of`em have some version of GNU Linux installed). The question was how to exploit such a treasury.</p>
<!--more-->

<p>And the answer is GNU Parallel with some additional bells and whistles. GNU Parallel allows one to execute some commands in parallel and even in a distributed way.</p>
<p>The command was as following</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">parallel</span> -u --wd ... -S :,host1,host2 --trc <span class="dt">{}</span>.emb <span class="st">&quot;sh {}&quot;</span></code></pre>
Here we have:
<ul>
	<li>
<strong>wd</strong> stands for working directory. Three-dots means <code>parallel</code>’s temporary folder
</li>
	<li>
<strong>S</strong> contains list of hosts with : being a localhost
</li>
	<li>
<strong>trc</strong> stands for “Transfer, Return, Cleanup” and means that we’d like to transfer an executable file to target host, return specified file and do a cleanup
</li>
</ul>

<p><code>parallel</code> accepts list command arguments (file names) in standard input and executes a command (<code>sh</code> in my case) for each of them.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ls</span> -1 jobs/* <span class="kw">|</span> <span class="kw">parallel</span> -u --wd ... -S :,host1,host2 --trc <span class="dt">{}</span>.emb <span class="st">&quot;sh {}&quot;</span></code></pre>
<p>There’s a problem: we usually need more than one file to do usefull stuff. There are several solutions to that problem</p>
<ul>
	<li>
<strong>Bring all files manually</strong><br/> It’s a solution, but somewhat tedious one: setting computing environment on a several machines is dull
</li>
	<li>
<strong>tar it and do all the stuff in a command</strong><br/> Looks better, but some shell kung-fu is required
</li>
	<li>
<strong>Use <a href="http://en.wikipedia.org/wiki/Shar">shar</a></strong><br/> Basically it’s a tar archive with some shell commands for (self-)extracting. I chose this way and glued in some my code.
</li>
</ul>

]]></description>
    <pubDate>Fri, 01 Aug 2014 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2014-08-01-gnu-parallel.html</guid>
</item>
<item>
    <title>On Sorting Complexity</title>
    <link>http://barmaley.exe.name/posts/2014-05-01-on-sorting-complexity.html</link>
    <description><![CDATA[<p>It’s well known that lower bound for sorting problem (in general case) is <span class="math">\(\Omega(n \log n)\)</span>. The proof I was taught is somewhat involved and is based on paths in “decision” trees. Recently I’ve discovered an information-theoretic approach (or reformulation) to that proof.</p>
<!--more-->

<p>First, let’s state the problem: given a set of some objects with an ordering produce elements of that set in that order. For now it’s completely irrelevant what are these objects, so we can assume them to be just numbers from 1 to n, or some permutation. Thus we’ll be interested in sorting permutations.</p>
<p>We’re given an ordering via a comparison function. It tells us if one object preceds (or is smaller) another outputing True or False. Thus each invocation of the comparator gives us 1 bit of information.</p>
<p>Next question is how many bits we need to represent any permutation. It’s just a binary logarithm of number of all possible permutations of <span class="math">\(n\)</span> elements: <span class="math">\(\log_2 n!\)</span>. Then we notice that</p>
<p><span class="math">\[
\log_2 n! = \sum_{k=1}^n \log_2 k \ge \sum_{k=n/2}^{n} \log_2 k
\ge \frac{n}{2} \log_2 \frac{n}{2}
\]</span></p>
<p><span class="math">\[
\log_2 n! = \sum_{k=1}^n \log_2 k \le n \log_2 n
\]</span></p>
<p>(Or just use the <a href="http://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a> formula). Hence <span class="math">\(\log_2 n! = \Theta(n \log n)\)</span></p>
<p>So what, you may ask. The key point of proof is that sorting is essentially a search for a correct permutation of the input one. Since one needs <span class="math">\(\log_2 n!\)</span> bits to represent any permutation, we <strong>need to get that many bits</strong> of information somehow. Now let’s get back to our comparison function. As we’ve figured out already it’s able to give us only one bit of information per invocation. That implies that we need to call it <span class="math">\(\log n! = \Theta(n \log n)\)</span> times. And that’s exactly the lower-bound for sorting complexity. Q.E.D.</p>
<p>Non-<span class="math">\(n \log n\)</span> sorting algorithms like <a href="http://en.wikipedia.org/wiki/Radix_sort">RadixSort</a> are possible because they use much more bits of information, taking advantage of numbers’ structure.</p>]]></description>
    <pubDate>Thu, 01 May 2014 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2014-05-01-on-sorting-complexity.html</guid>
</item>
<item>
    <title>Namespaced Methods in JavaScript</title>
    <link>http://barmaley.exe.name/posts/2013-05-23-js-namespaced-methods.html</link>
    <description><![CDATA[<p>Once upon a time I was asked (well, actually <a href="http://habrahabr.ru/qa/7130/" title="Javascript: String.prototype.namespace.method и this / Q&A / Хабрахабр">a question</a> wasn’t for me only, but for whole habrahabr’s community) is it possible to implement namespaced methods in JavaScript for built-in types like:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="dv">5</span>..<span class="ot">rubish</span>.<span class="fu">times</span>(<span class="kw">function</span>() { <span class="co">// this function will be called 5 times</span>
  <span class="ot">console</span>.<span class="fu">log</span>(<span class="st">&quot;Hi there!&quot;</span>);
});

<span class="st">&quot;some string&quot;</span>.<span class="ot">hask</span>.<span class="fu">map</span>(<span class="kw">function</span>(c) { <span class="kw">return</span> <span class="ot">c</span>.<span class="ot">hask</span>.<span class="fu">code</span>(); });
<span class="co">// equivalent to</span>
<span class="st">&quot;some string&quot;</span>.<span class="fu">split</span>(<span class="st">&#39;&#39;</span>).<span class="fu">map</span>(<span class="kw">function</span>(c) { <span class="kw">return</span> <span class="ot">c</span>.<span class="fu">charCodeAt</span>(); });

<span class="st">&quot;another string&quot;</span>.<span class="ot">algo</span>.<span class="fu">lcp</span>(<span class="st">&quot;annotation&quot;</span>); 
<span class="co">// returns longest common prefix of two strings</span></code></pre>
<p>As you can see at the link, it’s possible using ECMAScript 5 features. And that’s how: <!--more--></p>
<p>First, let’s point out the main problem with the straightforward approach: <del>it doesn’t work</del> when you write</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="ot">Class</span>.<span class="ot">prototype</span>.<span class="ot">ns</span>.<span class="fu">method</span> = <span class="kw">function</span>() {
  <span class="kw">return</span> <span class="kw">this</span>.<span class="fu">methodA</span>() + <span class="kw">this</span>.<span class="fu">methodB</span>();
}</code></pre>
<p><code>this</code> points to the <code>Class.prototype.ns</code> instead of an instance of <code>Class</code>. The only way to change it is rebind <code>this</code> to the our instance like this:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> instance = <span class="kw">new</span> Class;
<span class="ot">instance</span>.<span class="ot">ns</span>.<span class="ot">method</span>.<span class="fu">call</span>(instance);</code></pre>
<p>Obviously, it’s not a solution since in that case it is a lot easier to write something like</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> instance = <span class="kw">new</span> Class;
<span class="ot">MegaLibrary</span>.<span class="fu">method</span>(instance);</code></pre>
<p>Thus we need to somehow return a correct function (with <code>this</code> binded to the <code>instance</code>) when user calls namespaced methods. This can be done using <a href="http://stackoverflow.com/q/812961/1190430" title="Javascript getters and setters for dummies? - Stack Overflow" target="_blank">getters</a>.</p>
<p>When user accesses our namespace we give him a proxy-object with a custom getter for every method in the namespace. This getter returns a method with rebinded <code>this</code>. The question is: how do we get a reference to the <code>instance</code>? The answer is pretty simple: using getters again! Instead of declaring an ordinary property for the namespace we can create a property with a custom getter memoizing a reference to <code>this</code>. Voilà!</p>
Finally, the code is:
<script src="https://gist.github.com/Barmaley-exe/5599917.js"></script>

<h2 id="but-wait-how-cross-browser-is-it">But wait… How cross browser is it?</h2>
<p>Well, I’m pretty lazy to test it on all platforms (IE, Opera, FF, Chrome, Node.JS), so I’ll do like a mathematician in a famous anecdote:</p>
<blockquote>
<p>Three employees (an engineer, a physicist and a mathematician) are staying in a hotel while attending a technical seminar. The engineer wakes up and smells smoke. He goes out into the hallway and sees a fire, so he fills a trashcan from his room with water and douses the fire. He goes back to bed.</p>
<p>Later, the physicist wakes up and smells smoke. He opens his door and sees a fire in the hallway. He walks down the hall to a fire hose and after calculating the flame velocity, distance, water pressure, trajectory, etc. extinguishes the fire with the minimum amount of water and energy needed.</p>
Later, the mathematician wakes up and smells smoke. She goes to the hall, sees the fire and then the fire hose. She thinks for a moment and then exclaims, ‘Ah, a solution exists!’ and then goes back to bed.
</blockquote>

As you can see, the key part of code is ECMAScript 5’s <code>Object.defineProperty</code>. According to the kangax’s <a href="http://kangax.github.io/es5-compat-table/#Object.defineProperty" title="ECMAScript 5 compatibility table" target="_blank">ECMAScript 5 compatibility table</a> it has pretty good support:
<ul>
	<li>
IE 9+
</li>
	<li>
Opera 12+
</li>
	<li>
FF 4+
</li>
	<li>
Chrome 7+ (and thus Node.JS too)
</li>
</ul>

]]></description>
    <pubDate>Thu, 23 May 2013 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2013-05-23-js-namespaced-methods.html</guid>
</item>
<item>
    <title>Crazy Expression Parsing</title>
    <link>http://barmaley.exe.name/posts/2013-03-30-crazy-expression-parsing.html</link>
    <description><![CDATA[<p>Suppose we have an expression like <code>(5+5 * (x^x-5 | y &amp;&amp; 3))</code> and we’d like to get some computer-understandable representation of that expression, like:</p>
<p><code>ADD Token[5] MUL Token[5] AND BIT_OR XOR Token[x] SUB Token[x] Token[5] Token[y] Token[3]</code></p>
<p>In case if you don’t know how to do that or are looking for solutin right now, you should know that I’m not going to present right solution. This post is just a joke. You should use either <a href="http://en.wikipedia.org/wiki/Shunting-yard_algorithm" title="Shunting-yard algorithm — Wikipedia">Shunting-yard algorithm</a> or <a href="http://en.wikipedia.org/wiki/Recursive_descent_parser">Recursive descent parser</a>.</p>
<p>So if you’re ready for madness… Let’s go! <!--more--></p>
<p>Let’s take <a href="http://en.wikipedia.org/wiki/Don%27t_repeat_yourself">Don’t repeat yourself</a> principle as a justification. Moreover, let’s somewhat strengthen it to be “Don’t repeat”. Indeed, why do we need to repeat what compiler’s developers already did?</p>
Here we go
<script src="https://gist.github.com/Barmaley-exe/5273716.js"></script>

<p>In case you’re wondering what the heck is going on: all constants are converted to instances of <code>Token</code> class, for which I overloaded all the operators. Overloading is done in a way to preserve structure of an expression. The only thing we have to do then is to extract that information. In case you’re not familiar with C++, I recommend you to read something about operator overloading.</p>
<p>As you can see, g++ and python are required for that “parser”. Unfortunatelly, priority of a bitwise xor is too low to serve as a power operator.</p>]]></description>
    <pubDate>Sat, 30 Mar 2013 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2013-03-30-crazy-expression-parsing.html</guid>
</item>
<item>
    <title>Memoization Using C++11</title>
    <link>http://barmaley.exe.name/posts/2013-03-29-cpp-11-memoization.html</link>
    <description><![CDATA[<p>Recently I’ve read an article <a href="http://john-ahlgren.blogspot.ru/2013/03/efficient-memoization-using-partial.html" title="John Ahlgren: Efficient Memoization using Partial Function Application">Efficient Memoization using Partial Function Application</a>. Author explains function memoization using partial application. When I was reading the article, I thought “Hmmm, can I come up with a more general solution?” And as suggested in comments, one can use variadic templates to achieve it. So here is my version.</p>
<!--more-->
<p>First let’s do it in a more object-oriented way: we define a template class <code>Memoizator</code> with 2 parameters: a return value type and a list of argument’s types. Also we incapsulate a lookup map and will use C++11’s <a href="http://en.cppreference.com/w/cpp/utility/tuple" title="std::tuple - cppreference.com">std::tuple</a> to represent an arguments set.</p>
The code is as follows:
<script src="https://gist.github.com/Barmaley-exe/5270779.js"></script>

<p>Good, but what about computing n-th Fibonacci number using memoization? It’s not possible with a current version of <code>Memoizator</code> since it uses a separate map for each instance even if function is the same. It looks inefficient to store a separate lookup map for each instance of the same function. We’ll fix it by creating a static storage for maps accessed by a function address:</p>
<script src="https://gist.github.com/Barmaley-exe/5271223.js"></script>

<p>Now let’s compare the memoized version against the regular one. If we compute the 42th fibonacci number using simple recursive version (with exponential time complexity), we’d get</p>
<pre><strong>$ time ./a.out</strong> 
267914296

real    0m5.314s
user    0m5.220s
sys     0m0.020s</pre>
Now the memoized one (from the source above):
<pre><strong>$ time ./a.out</strong> 
267914296

real    0m0.005s
user    0m0.004s
sys     0m0.004s</pre>

<p>Moreover, our memoization reduced time complexity from exponential to linear.</p>
<p><strong>UPD</strong>: you can take a look at another implementation here: <a href="http://cpptruths.blogspot.ru/2012/01/general-purpose-automatic-memoization.html" title="c++ truths: General-purpose Automatic Memoization for Recursive Functions in C++11">General-purpose Automatic Memoization for Recursive Functions in C++11</a></p>]]></description>
    <pubDate>Fri, 29 Mar 2013 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2013-03-29-cpp-11-memoization.html</guid>
</item>
<item>
    <title>Resizing Policy of std::vector</title>
    <link>http://barmaley.exe.name/posts/2013-02-10-std-vector-growth.html</link>
    <description><![CDATA[Sometime ago when Facebook opensourced their <a title="Folly is an open-source C++ library developed and used at Facebook" href="https://github.com/facebook/folly">Folly library</a> I was reading their docs and found <a title="folly/FBvector.h documentation" href="https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md">something interesting</a>. In section “Memory Handling” they state
<blockquote>
In fact it can be mathematically proven that a growth factor of 2 is rigorously the worst possible because it never allows the vector to reuse any of its previously-allocated memory
</blockquote>
<p>I haven’t got it first time. Recently I recalled that article and decided to deal with it. So after reading and googling for a while I finally understood the idea, so I’d like to say a few words about it.</p>
<!--more-->

<p>The problem is as follows: when a vector (or a similar structure with autoresize) gets filled, it should resize. It’s well known that it should grow exponentially in order to preserve constant amortized complexity of insertions, but what growth factor to choose? At first glance, 2 seems to be ok — it’s not so big and 2 is common for computer science :-). But it turns out that 2 is not so good. Let’s take a closer look by example:</p>
<p><a href="/files/vector-resize-scheme.png"><img src="/files/vector-resize-scheme.png" alt="Vector resize scheme" width="495" height="350" class="size-full" /></a></p>
<p>Suppose we’ve a vector of initial size <span class="math">\(C\)</span>. When it gets filled, we increase its size twice. We allocate memory for a vector of size <span class="math">\(2C\)</span> right after our original vector. So now we have vector of size <span class="math">\(2C\)</span> and <span class="math">\(C\)</span> bytes before it, where it was when it was small. Then expand it again and again and agian and so on. After <span class="math">\(n\)</span> expansions we’ll get a vector of size <span class="math">\(2^n C\)</span> preceded by <span class="math">\(C + 2C + 2^2 C + \dots 2^{n-1} C\)</span> bytes that were occupied by this vector before.</p>
<p>So what’s the problem? The problem is that after every increasing your vector is too big to fit previously allocated memory. How much is it bigger? Well, as we know <span class="math">\(2^n - 1 = 1 + 2 + 4 + \dots + 2^{n-1}\)</span>, thus <span class="math">\(2^n C - C - 2C - 2^2 C - \dots - 2^{n-1}C = C\)</span>. Therefore you have permanent lack of <span class="math">\(C\)</span> bytes to fit your vector in previously allocated space.</p>
<p>Okay, let’s now solve this problem. First, let’s formalize it.</p>
<p>Every time we increase vector of size <span class="math">\(C\)</span> with growing factor <span class="math">\(k\)</span> we do these steps:</p>
<ol>
	<li>
Allocate <span class="math">\(k C\)</span> bytes
</li>
	<li>
Create a new vector here and copy current vector’s content to the new one
</li>
	<li>
Remove the current vector, set the new one as the current
</li>
</ol>

<p>So as you can see, formula for 2 is sort of upperbound: you can not use all of previously allocated <span class="math">\(n-1\)</span> chunks when allocating nth, since you need to copy values from (n-1)th (though you can copy them in some temporary buffer, but it requires extra memory) chunk. So when we allocate nth chunk, we need it to be less than total free space from <span class="math">\(n-2\)</span> allocations: <span class="math">\[ k^n C \le k^{n-2}C + k^{n-3}C + \dots + kC + C \]</span></p>
<p>As you can see, we can get rid of C since it’s definetly positive. <span class="math">\[ k^n \le k^{n-2} + k^{n-3} + \dots + k + 1 \]</span></p>
<p>Okay, time to solve some equations! We see something like a sum of a geometric progression, and we can use a formula for it. But I don’t retain it in my head, so I’ll use a little trick here. Let’s multiply both sides by <span class="math">\(k-1\)</span>. We assume that <span class="math">\(k &gt; 1\)</span> (it’s very strange to use values greater than 1 as <em>growth</em> factor) <span class="math">\[ (k-1) k^n \le (k-1) (k^{n-2} + k^{n-3} + \dots + k + 1) \]</span></p>
<p>Now we can notice that in the right side we have an expansion of <span class="math">\(k^{n-1}-1\)</span> (well, maybe to remember this observation is harder than remembering a formula for sum of a geometric progression…)</p>
<p><span class="math">\[ (k-1) k^n \le k^{n-1}-1 \]</span> <span class="math">\[ k^{n+1} - k^n \le k^{n-1}-1 \]</span> <span class="math">\[ k^{n+1} \le k^n + k^{n-1} - 1 \]</span></p>
<p>Oh, this obstructing 1… It would be so nice if we could throw it away! Wait, but we can! If we add 1 to the right side, we will merely increase its value, so it will still suit for an upper bound (or an approximation since 1 is a constant and is very small compared to <span class="math">\(k^n\)</span>).</p>
<p><span class="math">\[ k^{n+1} \le k^n + k^{n-1} - 1 &lt; k^n + k^{n-1} \]</span> <span class="math">\[ k^2 &lt; k + 1 \]</span> <span class="math">\[ k^2 - k - 1 &lt; 0 \]</span></p>
<p>Solving this simple quadratic equation, we get <span class="math">\[ k &lt; \frac{1+\sqrt{5}}{2} \approx 1.61 \]</span></p>
<p>So that’s why growth factor in many dynamic arrays is 1.5: it is pretty big to not cause reallocations too frequently and is small enough to not use memory too extensively.</p>]]></description>
    <pubDate>Sun, 10 Feb 2013 00:00:00 UT</pubDate>
    <guid>http://barmaley.exe.name/posts/2013-02-10-std-vector-growth.html</guid>
</item>

    </channel> 
</rss>
