<!DOCTYPE html>
<html lang="en">
<head>
  <title>Neural Variational Inference: Scaling Up - B.log</title>
  <meta charset="utf-8" />
  <link rel="shortcut icon" href="../favicon.ico" />

  <link rel="stylesheet" type="text/css" href="../css/default.css" />
  <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="../">B.log</a></h1>
        <h2>Random notes on Computer Science, Mathematics and Software Engineering</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="../">Home</a>
          <a href="../pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
        <article>
<header>
	<h3>Neural Variational Inference: Scaling Up</h3>
	<time>July  4, 2016</time>
</header>

<section>
<p>In the previous post I covered well-established classical theory developed in early 2000-s. Since then technology has made huge progress: now we have much more data, and a great need to process it and process it fast. In big data era we have huge datasets, and can not afford too many full passes over it, which might render classical VI methods impractical. Recently M. Hoffman et al. dissected classical Mean-Field VI to introduce stochasticity right into its heart, which resulted in <a href="https://arxiv.org/abs/1206.7051">Stochastic Variational Inference</a>.</p>
<!--more-->

<h3>
Stochastic Variational Inference
</h3>

<p>We start with model assumptions: we have 2 types of latent variables, the global latent variable <span class="math">\(\beta\)</span> and a bunch of local variables <span class="math">\(z_n\)</span> for each observation <span class="math">\(x_n\)</span>. Recalling our GMM example, <span class="math">\(\beta\)</span> can be thought of as a mixture weights <span class="math">\(\pi\)</span>, and <span class="math">\(z_n\)</span> are membership indicators, as previously. These variables are assumed to come from some exponential family distribution:</p>
<p><span class="math">\[
p(x_n, z_n \mid \beta) = h(x_n, z_n) \exp \left( \beta^T t(x_n, z_n) - a_l(\beta) \right) \\
\\
p(\beta) = h(\beta) \exp(\alpha^T t(\beta) - a_g(\alpha))
\]</span></p>
<p>Where <span class="math">\(t(\cdot)\)</span> and <span class="math">\(h(\cdot)\)</span> are overloaded by their argument, so <span class="math">\(t(\beta)\)</span> and <span class="math">\(t(z_{nj})\)</span> correspond to two different functions. <span class="math">\(t(\cdot)\)</span> gives a <strong>natural parameter</strong> and also <strong>sufficient statistics</strong>. <span class="math">\(a_g\)</span> and <span class="math">\(a_l\)</span> are log-normalizing constants which for exponential family distributions have an interesting property, namely, the gradient of the log-normalizing constant is the expectation of sufficient statistics: <span class="math">\(\nabla_\alpha a_g(\alpha) = \mathbb{E} t(\beta)\)</span>.</p>
<p>From these assumptions we can derive <em>complete conditionals</em> (conditional distribution given all other hidden variables and observables) for <span class="math">\(\beta\)</span> and <span class="math">\(z_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
p(\beta \mid x, z, \alpha)
&amp;\propto \prod_{n=1}^N p(x_n, z_n \mid \beta) p(\beta \mid \alpha) \\
&amp;= h(\beta) \prod_{n=1}^N h(x_n, z_n) \exp \left( \beta^T \sum_{n=1}^N t(x_n, z_n) - N a_l(\beta) + \alpha^T t(\beta) - a_g(\alpha) \right) \\
&amp;\propto h(\beta) \exp \left( \eta_g(x, z, \alpha)^T t(\beta) \right)
\end{align}
\]</span></p>
<p>Where <span class="math">\(t(\beta) = (\beta, -a_l(\beta))\)</span>, <span class="math">\(\eta_g(x, z, \alpha) = (\alpha_1 + \sum_{n=1}^N t(x_n, z_n), \alpha_2 + N)\)</span>. We see that the (unnormalized) posterior distribution for <span class="math">\(\beta\)</span> has the same functional form as the (unnormalized) prior <span class="math">\(p(\beta)\)</span>, therefore after normalization it’d be</p>
<p><span class="math">\[
p(\beta \mid x, z, \alpha)
= h(\beta) \exp \left( \eta_g(x, z, \alpha)^T t(\beta) - a_g(\eta_g(x, z, \alpha)) \right)
\]</span></p>
<p>The same applies to local variables <span class="math">\(z_{nj}\)</span>:</p>
<p><span class="math">\[
p(z_{nj} \mid x_n, z_{n,-j}, \beta)
\propto h(z_{nj}) \exp \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) \right)
\]</span> Hence <span class="math">\[
p(z_{nj} \mid x_n, z_{n,-j}, \beta)
= h(z_{nj}) \exp \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) - a_m(\eta_l(x_n, z_{n,-j}, \beta)) \right)
\]</span></p>
<p>Even though we’ve managed to find the complete conditional for <span class="math">\(\beta\)</span>, it might be intractable to find the posterior for all latent variables <span class="math">\(p(\beta, z \mid x, \alpha)\)</span>. We therefore turn to the mean field approximation:</p>
<p><span class="math">\[
q(z, \beta \mid \Lambda) = q(\beta \mid \lambda) \prod_{n=1}^N \prod_{j=1}^J q(z_{nj} \mid \phi_{nj})
\]</span></p>
<p>We assume these marginal distributions come from the exponential family:</p>
<p><span class="math">\[
q(\beta \mid \lambda) = h(\beta) \exp(\lambda^T t(\beta) - a_g(\lambda)) \\
q(z_{nj} \mid \phi_{nj}) = h(z_{nj}) \exp(\phi_{nj}^T t(z_{nj}) - a_m(\phi_{nj}))
\]</span></p>
<p>Let’s find the optimal variational parameters now by optimizing the ELBO <span class="math">\(\mathcal{L}(\Theta, \Lambda)\)</span> (<span class="math">\(\Theta\)</span> is model parameters, <span class="math">\(\alpha\)</span>, and <span class="math">\(\Lambda\)</span> contains variational parameters <span class="math">\(\phi\)</span> and <span class="math">\(\lambda\)</span>) by <span class="math">\(\lambda\)</span> and <span class="math">\(\phi_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\lambda)
&amp;= \mathbb{E}_{q} \left( \log p(x, z, \beta) - \log q(\beta) - \log q(z) \right)
= \mathbb{E}_{q} \left( \log p(\beta \mid x, z) - \log q(\beta) \right) + \text{const} \\
&amp;= \mathbb{E}_{q} \left( \eta_g(x, z, \alpha)^T t(\beta) - \lambda^T t(\beta) + a_g(\lambda) \right) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)^T \mathbb{E}_{q(\beta)} t(\beta) + a_g(\lambda) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)^T \nabla_\lambda a_g(\lambda) t(\beta) + a_g(\lambda) + \text{const}
\end{align}
\]</span></p>
<p>Where we used aforementioned property of exponential family distributions: <span class="math">\(\nabla_\lambda a_g(\lambda) = \mathbb{E}_{q(\beta)} t(\beta)\)</span>. The gradient then is <span class="math">\[
\nabla_\lambda \mathcal{L}(\lambda)
= \nabla_\lambda^2 a_g(\lambda) \left(\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \right)
\]</span></p>
<p>After setting it to zero we get an update for global latent variables: <span class="math">\(\lambda = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha)\)</span>. Following the same reasoning we derive the optimal update for <span class="math">\(\phi_{nj}\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\phi_{nj})
&amp;= \mathbb{E}_{q} \left( \log p(z_{nj} \mid x_n, z_{n,-j}, \beta) - \log q(z_{nj}) \right) + \text{const} \\
&amp;= \mathbb{E}_{q} \left( \eta_l(x_n, z_{n,-j}, \beta)^T t(z_{nj}) - \phi_{nj}^T t(z_{nj}) + a_m(\phi_{nj})\right) + \text{const} \\
&amp;= \left(\mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} \right)^T \mathbb{E}_{q(z_{nj})} t(z_{nj}) + a_m(\phi_{nj}) + \text{const} \\
\end{align}
\]</span></p>
<p>The gradient then is <span class="math">\(\nabla_{\phi_{nj}} \mathcal{L}(\phi) = \nabla_{\phi_{nj}}^2 a_m(\phi_{nj}) \left(\mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} \right)\)</span>, and the update is <span class="math">\(\phi_{nj} = \mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta)\)</span>.</p>
<p>So far we found mean-field updates, as well as corresponding gradients of the ELBO for variational parameters <span class="math">\(\lambda\)</span> and <span class="math">\(\phi_{nj}\)</span>. Next step is to transform these gradients into <strong>natural gradients</strong>. Intuitively, classical gradient defines local linear approximation, where the notion of locality comes from the Euclidean space. However, parameters influence the ELBO only through distributions <span class="math">\(q\)</span>, so we might like to alter our idea of locality based on how much the distributions change. This is what natural gradient does: it defines local linear approximation where locality means small distance (symmetrized KL-divergence) between distributions. There’s great formal explanation in the paper, and if you want to read more on that matter, I refer you to a great post by Roger Grosse, <a href="http://www.metacademy.org/roadmaps/rgrosse/dgml">Differential geometry for machine learning</a>.</p>
<p>The natural gradient can be obtained from the usual gradient using a simple linear transformation:</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} f(\lambda) = \mathcal{I}(\lambda)^{-1} \nabla_{\lambda} f(\lambda)
\]</span></p>
<p>Where <span class="math">\(\mathcal{I}(\lambda) := \mathbb{E}_{q(\beta \mid \lambda)} \left[ \nabla_\lambda \log q(\beta \mid \lambda) (\nabla_\lambda \log q(\beta \mid \lambda))^T \right]\)</span> is Fisher Information Matrix. Here I considered parameter <span class="math">\(\lambda\)</span> of the distribution <span class="math">\(q(\beta \mid \lambda)\)</span>, you got the idea. For the exponential family distribution this Information Matrix takes an especially simple form:</p>
<p><span class="math">\[
\begin{align}
\mathcal{I}(\lambda)
&amp;= \mathbb{E}_q (t(\beta) - \nabla_\lambda a_g(\lambda)) (t(\beta) - \nabla_\lambda a_g(\lambda))^T
= \mathbb{E}_q (t(\beta) - \mathbb{E}_q t(\beta)) (t(\beta) - \mathbb{E}_q t(\beta))^T \\
&amp;= \text{Cov}_q (t(\beta))
 = \nabla_\lambda^2 a_g(\lambda) 
\end{align}
\]</span></p>
<p>Where we’ve used another <a href="https://en.wikipedia.org/wiki/Exponential_family#Differential_identities_for_cumulants">differential identity for exponential family</a>. All these calculations lead us to the natural gradients of ELBO for variational parameters:</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} \mathcal{L}(\lambda) = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda \\
\nabla_{\phi_{nj}}^\text{N} \mathcal{L}(\lambda) = \mathbb{E}_{q(\beta) q(z_{n,-j})} \eta_l(x_n, z_{n,-j}, \beta) - \phi_{nj} 
\]</span></p>
<p>Surprisingly, computation-wise calculating natural gradients is even simpler that calculating classical gradients! There’s an interesting connection between the mean-field update and a natural gradient step. In particular, if we make a step along the natural gradient with step size equal 1, we’d get <span class="math">\(\lambda^{\text{new}} = \lambda^{\text{old}} + (\mathbb{E}_{q(z)} \eta_g(x, z, \alpha) - \lambda^{\text{old}}) = \mathbb{E}_{q(z)} \eta_g(x, z, \alpha)\)</span>. The same applies to parameters <span class="math">\(\phi\)</span>. This means that the mean field updates are exactly natural gradient steps, and vice versa.</p>
<p>Recall, we have derived mean field updates by finding a minima of KL-divergence with the true posterior, that is in just one step (one update) we arrive at minimum. Obviously, we have the same in the natural gradient formulation, when just one step brings us to the optimum.</p>
<p>Now, the last component is stochasticity itself. So far we have only played a little with mean-field update scheme, and discovered its connection to the natural gradient optimization. We note that we have 2 parameters: local <span class="math">\(\phi_{nj}\)</span> and global parameter <span class="math">\(\lambda\)</span>. The first one is easy to optimize over as it depends only on one, <span class="math">\(n\)</span>th sample <span class="math">\(x_n\)</span>. The second one, though, needs to incorporate information from all the samples, which is computationally prohibitive in large scale regime. Luckily, now once we know the equivalence between mean-field update and natural gradient step, we can borrow ideas from stochastic optimization to make this process more scalable.</p>
<p>Let’s first reformulate the ELBO to include the sum over samples <span class="math">\(x_n\)</span>:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}(\Theta, \Lambda)
&amp;= \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \sum_{n=1}^N \left(\log p(x_n, z_n \mid \beta) - \log q(z_n \mid \phi_n) \right) \right] \\
&amp; = \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + N \mathbb{E}_{I} \left(\log p(x_I, z_I \mid \beta) - \log q(z_I \mid \phi_I) \right) \right]
\end{align}
\]</span></p>
<p>Where <span class="math">\(I \sim \text{Unif}\{1, \dots, N\}\)</span> — uniformly distribution index of a sample. Now let’s estimate <span class="math">\(\mathcal{L}\)</span> using a sample <span class="math">\(S\)</span> (assume <span class="math">\(N\)</span> divides by sample size <span class="math">\(|S|\)</span>) of uniformly chosen indices, this’d result in an unbiased estimator (it’s gradient would also be unbiased, so we can maximize the true ELBO by maximizing the estimate). Author of the paper start with single-sample derivation and then extend it to minibatches, but I decided I’d go straight to the minibatch case:</p>
<p><span class="math">\[
\begin{align}
\mathcal{L}_S(\Theta, \Lambda)
&amp; := \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \frac{N}{|S|} \sum_{i \in S} \left(\log p(x_i, z_i \mid \beta) - \log q(z_i \mid \phi_i) \right) \right] \\
&amp; = \mathbb{E}_{q} \left[ \log p(\beta \mid \alpha) - \log q(\beta \mid \lambda) + \sum_{n=1}^{N / |S|} \sum_{i \in S} \left(\log p(x_i, z_i \mid \beta) - \log q(z_i \mid \phi_i) \right) \right]
\end{align}
\]</span></p>
<p>This estimate is exactly <span class="math">\(\mathcal{L}(\Theta, \Lambda)\)</span> calculated on sample consisting of <span class="math">\(\{x_i, z_i\}_{i \in S}\)</span> repeated <span class="math">\(N / |S|\)</span> times. Hence its natural gradient w.r.t. <span class="math">\(\lambda\)</span> is</p>
<p><span class="math">\[
\nabla_\lambda^\text{N} \mathcal{L}_S(\lambda) = \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha) - \lambda \\
\]</span></p>
<p>One important note: for stochastic optimization we can’t use constant step size. As Robbins-Monro conditions suggest, we need to use schedule <span class="math">\(\rho_t\)</span> such that <span class="math">\(\sum \rho_t = \infty\)</span> and <span class="math">\(\sum \rho_t^2 &lt; \infty\)</span>. Then the update <span class="math">\(\lambda^{\text{new}} = \lambda^{\text{old}} + \rho_t \nabla_\lambda^\text{N} \mathcal{L}_S(\lambda) = (1 - \rho_t) \lambda^{\text{old}} + \rho_t \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha)\)</span></p>
<p>Finally we have the following optimization scheme:</p>
<ul>
	<li>
Start with random initialization for <span class="math">\(\lambda^{(0)}\)</span>
</li>
	<li>
For <span class="math">\(t\)</span> from 0 to MAX_ITER
<ol>
		<li>
Sample <span class="math">\(S \sim \text{Unif}\{1, \dots, N\}^{|S|}\)</span>
</li>
		<li>
For each sample <span class="math">\(i \in S\)</span> update the local variational parameter <span class="math">\(\phi_{i,j} = \mathbb{E}_{q(\beta) q(z_{i,-j})} \eta_l(x_i, z_{i,-j}, \beta)\)</span>
</li>
		<li>
Replicate the sample <span class="math">\(N / |S|\)</span> times and compute the global update <span class="math">\(\hat \lambda = \mathbb{E}_{q(z)} \eta_g(\{x_S\}_{n=1}^{N/|S|}, \{z_S\}_{n=1}^{N/|S|}, \alpha)\)</span>
</li>
		<li>
Update the global update <span class="math">\(\lambda^{(t+1)} = (1-\rho_t) \lambda^{(t)} + \rho_t \hat \lambda\)</span>
</li>
	</ol>
	</li>
</ul>


</section>

<div class="tags-pane">Tagged as <a href="../tags/machine%20learning.html">machine learning</a>, <a href="../tags/deep%20learning.html">deep learning</a>, <a href="../tags/variational%20inference.html">variational inference</a></div>
</article>

<section>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'barmaley-exe'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</section>

    </section>
    <footer>
        <a href="http://jaspervdj.be/hakyll/index.html">Generated with Hakyll</a>
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
