<!DOCTYPE html>
<html lang="en">
<head>
  <title>Neural Variational Inference: Importance Weighted Autoencoders - B.log</title>
  <meta charset="utf-8" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta name="twitter:title" content="Neural Variational Inference: Importance Weighted Autoencoders" />
  <meta name="twitter:description" content="Previously we covered Variational Autoencoders (VAE) — popular inference tool based on neural networks. In this post we’ll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, Importance Weighted Autoencoders (IWAE). The crucial contribution of this work is introduction of a new lower-bound on the marginal log-likelihood \(\log p(x)\) which generalizes ELBO, but also allows one to use less accurate approximate posteriors \(q(z \mid x, \Lambda)\).
On a dessert we’ll discuss another paper, Variational inference for Monte Carlo objectives by A. Mnih and D. Rezende which aims to broaden the applicability of this approach to models where reparametrization trick can not be used (e.g. for discrete variables).
" /> 

  <link rel="shortcut icon" href="../favicon.ico" />

  <link rel="stylesheet" type="text/css" href="../css/default.css" />
  <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:,b" />
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Register.StartupHook("TeX Jax Ready", function () {
      MathJax.InputJax.TeX.Definitions.Add({
          macros: {
              E: ["Macro", "\\mathop{\\mathbb{E}}"]
          }
      });
  });
  </script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="../">B.log</a></h1>
        <h2>Random notes mostly on Machine Learning</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="../">Home</a>
          <a href="../pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
        <article>
<header>
	<h1>Neural Variational Inference: Importance Weighted Autoencoders</h1>
	<time>July 14, 2016</time>
</header>

<section class="post-body">
<p>Previously we covered <a href="../posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">Variational Autoencoders</a> (VAE) — popular inference tool based on neural networks. In this post we’ll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, <a href="https://arxiv.org/abs/1509.00519">Importance Weighted Autoencoders</a> (IWAE). The crucial contribution of this work is introduction of a new lower-bound on the marginal log-likelihood <span class="math inline">\(\log p(x)\)</span> which generalizes ELBO, but also allows one to use less accurate approximate posteriors <span class="math inline">\(q(z \mid x, \Lambda)\)</span>.</p>
<p>On a dessert we’ll discuss another paper, <a href="https://arxiv.org/abs/1602.06725">Variational inference for Monte Carlo objectives</a> by A. Mnih and D. Rezende which aims to broaden the applicability of this approach to models where reparametrization trick can not be used (e.g. for discrete variables).</p>
<!--more-->
<h3>
Importance Weighted Autoencoders
</h3>
<p>Let’s first answer the question of how one can come up with a lower bound for the marginal log-likelihood? In the very beginning of the series, <a href="../posts/2016-07-01-neural-variational-inference-classical-theory.html">Classical Theory</a> post, we used some trickery to come up with the ELBO. That massaging of the marginal log-likelihood wasn’t particulary enlightening on how one could invent that lower bound. Now we’re going to consider a principled approach to invention of new lower bounds based on <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a>.</p>
<p>Suppose we have some unbiased estimator <span class="math inline">\(f(x, z)\)</span> of <span class="math inline">\(p(x)\)</span>, that is, <span class="math inline">\(\mathbb{E}_z f(x, z) = p(x)\)</span>. Then</p>
<p><span class="math display">\[
\log p(x) = \log \mathbb{E}_z f(x, z) \stackrel{\text{Jensen}}{\ge} \mathbb{E}_z \log f(x, z)
\]</span></p>
<p>In particular, if <span class="math inline">\(z \sim q(z \mid x)\)</span> and <span class="math inline">\(f(x, z) = \tfrac{p(x, z)}{q(z \mid x)}\)</span>, we obtain the standard ELBO. The IWAE paper proposes another estimate (actually, a family of estimators parametrized by an integer <span class="math inline">\(K\)</span>) of marginal <span class="math inline">\(p(x)\)</span>:</p>
<p><span class="math display">\[
f(x, z_1, \dots, z_K) = \frac{1}{K} \sum_{k=1}^K \frac{p(x, z_k)}{q(z_k \mid x)}
\]</span></p>
<p>Where each <span class="math inline">\(z_k\)</span> comes from the same distribution <span class="math inline">\(q(z_k \mid x) = q(z \mid x)\)</span>. Obviously, <span class="math inline">\(f(x, z_1, \dots, z_K)\)</span> is still an unbiased estimator of the <span class="math inline">\(p(x)\)</span>, and therefore <span class="math inline">\(\mathbb{E}_z \log f(x, z_1, \dots, z_K)\)</span> is a valid lower-bound on the marginal log-likelihood.</p>
<p>Let’s analyze this new lower-bound now. First, let’s dissect the ELBO:</p>
<p><span class="math display">\[
\mathcal{L}(\Theta, \Lambda)
= \mathbb{E}_q \log \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)}
= \mathbb{E}_q \left[\log \frac{p(z \mid x, \Theta)}{q(z \mid x, \Lambda)} \right] + \log p(x \mid \Theta)
\]</span></p>
<p>If <span class="math inline">\(q\)</span> approximates the true posterior accurately, the first term (which is a KL-divergence, BTW) is close to zero. However, when estmating it using Monte Carlo samples, the ELBO heavily penalizes inaccurate approximations: if <span class="math inline">\(q(z \mid x, \Lambda)\)</span> gives us samples from high probability regions of the true posterior <span class="math inline">\(p(z \mid x, \Theta)\)</span> only occasionally (like 20% of times), the gap between the ELBO and the marginal log-likelihood would be huge (<span class="math inline">\(p(z\mid x, \Theta)\)</span> is small, <span class="math inline">\(q(z \mid x, \Lambda)\)</span> is big), which does not help learning. As you might have guessed, IWAE allows us to use several samples. Let’s see it in detail:</p>
<p><span class="math display">\[
\mathcal{L}_K(\Theta, \Lambda)
:= \mathbb{E}_q \left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(x, z_k \mid \Theta)}{q(z_k \mid x, \Lambda)} \right]
:= \mathbb{E}_q \left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(z_k \mid x, \Theta)}{q(z_k \mid x, \Lambda)} \right] + \log p(x \mid \Theta)
\]</span></p>
<p>This averaging of posterior ratios saves us from bad samples screwing the lower bound, as it’ll be pushed up by good samples (provided the approximation has a reasonable probability of generating a good sample in <span class="math inline">\(K\)</span> attempts). This allows one to perform model inference even with poor approximations <span class="math inline">\(q(z \mid x, \Lambda)\)</span>. The more samples <span class="math inline">\(K\)</span> we use — the less accurate approximation we can tolerate. In fact, authors prove the following theorem:</p>
<blockquote>
<p><strong>Theorem 1</strong>. For all <span class="math inline">\(K\)</span>, the lower bounds satisfy <span class="math display">\[
\log p(x \mid \Theta) \ge \mathcal{L}_{K+1}(\Theta, \Lambda) \ge \mathcal{L}_{K}(\Theta, \Lambda)
\]</span></p>
Moreover, if <span class="math inline">\(p(z, x \mid \Theta) / q(z \mid x, \Lambda)\)</span> is bounded, then <span class="math inline">\(\mathcal{L}_{K}(\Theta, \Lambda)\)</span> approaches <span class="math inline">\(\log p(x \mid \Theta)\)</span> as <span class="math inline">\(K\)</span> goes to infinity.
</blockquote>
<p>The convergence result follows from the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law">strong law of large numbers</a>.</p>
<p>As with VAE, we use the reparametrization trick to avoid backpropagation through stochastic units:</p>
<p><span class="math display">\[
\mathcal{L}_K(\Theta, \Lambda) = \mathbb{E}_{\varepsilon_1, \dots, \varepsilon_K} \log \frac{1}{K} \sum_{k=1}^K \overbrace{\frac{p(x, g(\varepsilon_k; \Lambda) \mid \Theta)}{q(g(\varepsilon_k; \Lambda) \mid x, \Lambda)}}^{w(x, \varepsilon_k, \Theta, \Lambda)}
\]</span></p>
<p>The gradients then are</p>
<p><span class="math display">\[
\nabla_\Theta \mathcal{L}_K(\Theta, \Lambda) = \mathbb{E}_{\varepsilon_1, \dots, \varepsilon_K} \sum_{k=1}^K \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) \nabla_\Theta \log w(x, \varepsilon_k, \Theta, \Lambda) \\
\nabla_\Lambda \mathcal{L}_K(\Theta, \Lambda) = \mathbb{E}_{\varepsilon_1, \dots, \varepsilon_K} \sum_{k=1}^K \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) \nabla_\Lambda \log w(x, \varepsilon_k, \Theta, \Lambda) \\
\text{where } \hat w_k(x, \varepsilon_{1 \dots K}, \Theta, \Lambda) := \frac{w(x, \varepsilon_k, \Theta, \Lambda)}{\sum_{k=1}^K w(x, \varepsilon_k, \Theta, \Lambda)}
\]</span></p>
<p>(We used the identity <span class="math inline">\(\nabla_x f(x) = f(x) \nabla_x \log f(x)\)</span> here).</p>
<p>Just as one would expect, setting <span class="math inline">\(K=1\)</span> reduces these gradients to ones we’ve seen in VAEs as the only importance weight <span class="math inline">\(\hat w_1\)</span> is equal to 1. Unfortunatelly, this approach does not allow one to decompose the lower-bound into the reconstruction error and KL-divergence to analytically compute the later. However, authors report indistinguishable performance of 2 approaches (with KL computed analytically or estimated using Monte Carlo) in case of <span class="math inline">\(K=1\)</span>.</p>
<p>BTW, <a href="http://info.usherbrooke.ca/hlarochelle/index_en.html">Hugo Larochelle</a> writes <a href="https://twitter.com/hugo_larochelle/timelines/639067398511968256">notes</a> on different papers, and he has written and made publicly available <a href="https://www.evernote.com/shard/s189/sh/e2c8a133-1814-474c-b267-366600a1921b/06a756f1618cd47ababc7aae0e514dbf">Notes on Importance Weighted Autoencoders</a>.</p>
<h3>
Variational inference for Monte Carlo objectives
</h3>
<p>As I said in the introduction, IWAE has been “generalized” to discrete variables — a case when one can not employ the reparametrization trick, and instead has to somehow reduce high variance of a score function-based estimator. Previously, during our discussion of the <a href="../posts/2016-07-05-neural-variational-inference-blackbox.html">Blackbox VI and variance reduction techniques</a> we covered NVIL (Neural Variational Inference and Learning) estimator, which uses another neural network to estimate marginal likelihood and reduce the variance. This work is built upon a similar idea.</p>
<p>First, let’s derive score-function-based gradients for variational parameters <span class="math inline">\(\Lambda\)</span> (where <span class="math inline">\(w\)</span> now is defined as <span class="math inline">\(w(x, z, \Theta, \Lambda) = \frac{p(x, z \mid \Theta)}{q(z \mid x, \Lambda)}\)</span>, and <span class="math inline">\(\hat w\)</span> is a normalized across all samples <span class="math inline">\(z_{1\dots K}\)</span> version):</p>
<p><span class="math display">\[
\begin{align}
\nabla_\Lambda \mathcal{L}_K(\Theta, \Lambda)
&amp;= \nabla_\Lambda \mathbb{E}_{q(z_1, \dots, z_K \mid x, \Lambda)} \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \\
&amp;= \nabla_\Lambda \int q(z_1, \dots, z_K \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \; dz_1 \dots dz_K \\
&amp;= \mathbb{E}_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \nabla_\Lambda \log q(z_k \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \right] \\
&amp; \quad+ \mathbb{E}_{q(z_1, \dots, z_K \mid x, \Lambda)} \nabla_\Lambda \log \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \\
&amp;= \mathbb{E}_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \nabla_\Lambda \log q(z_k \mid x, \Lambda) \log \frac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda) \right] \\
&amp; \quad+ \mathbb{E}_{q(z_1, \dots, z_K \mid x, \Lambda)} \left[ \sum_{k=1}^K \hat w_k(x, z_{1 \dots K}, \Theta, \Lambda) \nabla_\Lambda \log w(x, z_k, \Theta, \Lambda) \right]
\end{align}
\]</span></p>
<p>The second term is exactly the gradient of the reparametrized case, and it does not cause us any troubles. The first term, however has some issues.</p>
<p>First, it does not distinguish individual samples’ contributions: indeed, gradients for all samples have the same weight of <span class="math inline">\(\log \tfrac{1}{K} \sum_{k=1}^K w(x, z_k, \Theta, \Lambda)\)</span> (called the <em>learning signal</em>) regardless of how probable they’re in terms of the true posterior (that is, how well they describe an observation <span class="math inline">\(x\)</span>). Compare it with the second term, where gradient for each sample <span class="math inline">\(z_k\)</span> is weighted in proportion to its importance weight <span class="math inline">\(\hat w_k\)</span>.</p>
<p>Second problem is that the learning signal is unbounded, and can be quite high. Again, the second term does not suffer this as importance weights <span class="math inline">\(\hat w_k\)</span> are normalized to sum to 1.</p>
<p>One can use the NVIL estimator we’ve discussed previously to reduce the variance due to large magnitude of a learning signal. However, it does not address the problem of all gradients having the same weight. For this the authors propose to introduce per-sample baselines that minimize dependencies between samples.</p>
<p>This paper has also caught Dr. Larochelle’s attention: <a href="https://www.evernote.com/shard/s189/sh/54a9fb88-1a71-4e8a-b0e3-f13480a68b8d/0663de49b93d397f519c7d7f73b6a441">Notes on Variational inference for Monte Carlo objectives</a>.</p>
</section>

<div class="tags-pane">Tagged as <a href="../tags/machine%20learning.html">machine learning</a>, <a href="../tags/deep%20learning.html">deep learning</a>, <a href="../tags/variational%20inference.html">variational inference</a>, <a href="../tags/modern%20variational%20inference%20series.html">modern variational inference series</a></div>
</article>

<section class="post-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'barmaley-exe'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

    </section>
    <footer>
        <a href="http://jaspervdj.be/hakyll/index.html">Generated with Hakyll</a>
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
