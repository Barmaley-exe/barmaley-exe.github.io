<!DOCTYPE html>
<html lang="en">
<head>
  <title>Stochastic Computation Graphs: Continuous Case - B.log</title>
  <meta charset="utf-8" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta name="twitter:title" content="Stochastic Computation Graphs: Continuous Case" />

  <meta name="twitter:description" content="Last year I covered some modern Variational Inference theory. These methods are often used in conjunction with Deep Neural Networks to form deep generative models (VAE, for example) or to enrich deterministic models with stochastic control, which leads to better exploration. Or you might be interested in amortized inference.
All these cases turn your computation graph into a stochastic one – previously deterministic nodes now become random. And it’s not obvious how to do backpropagation through these nodes. In this series I’d like to outline possible approaches. This time we’re going to see why general approach works poorly, and see what we can do in continuous case.
" />


  <link rel="shortcut icon" href="../favicon.ico" />

  <link rel="stylesheet" type="text/css" href="../css/default.css" />
  <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="../">B.log</a></h1>
        <h2>Random notes on Computer Science, Mathematics and Software Engineering</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="../">Home</a>
          <a href="../pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
        <article>
<header>
	<h3>Stochastic Computation Graphs: Continuous Case</h3>
	<time>September 10, 2017</time>
</header>

<section class="post-body">
<p>Last year I covered <a href="../tags/modern%20variational%20inference%20series.html">some modern Variational Inference theory</a>. These methods are often used in conjunction with Deep Neural Networks to form deep generative models (VAE, for example) or to enrich deterministic models with stochastic control, which leads to better exploration. Or you might be interested in amortized inference.</p>
<p>All these cases turn your computation graph into a stochastic one – previously deterministic nodes now become random. And it’s not obvious how to do backpropagation through these nodes. In <a href="../tags/stochastic%20computation%20graphs%20series.html">this series</a> I’d like to outline possible approaches. This time we’re going to see why general approach works poorly, and see what we can do in continuous case.</p>
<!--more-->
<p>First, let’s state the problem more formally. Consider the approximate inference objective:</p>
<p><span class="math display">\[
\mathbb{E}_{q(z|x)} \log \frac{p(x, z)}{q(z|x)} \to \max_{q(z|x)}
\]</span></p>
<p>or reinforcement learning objective</p>
<p><span class="math display">\[
\mathbb{E}_{\pi(a|s)} R(a, s) \to \max_{\pi}
\]</span></p>
<p>In the following I’ll denote our objective as</p>
<p><span class="math display">\[
\mathcal{F}(\theta) = \mathbb{E}_{p(x \mid \theta)} f(x) \to \max_{\theta}
\]</span></p>
<p>In that case the (stochastic) computation graph (SCG) can be represented in the following form:</p>
<div class="post-image">
<p><img src="../files/scg-through-randomness.png" style="width: 400px" /></p>
</div>
<p>Here <span class="math inline">\(\theta\)</span>, in double circle is a set of tunable parameters, blue rhombus is a stochastic node that takes some random values, but their distribution depends on <span class="math inline">\(\theta\)</span> (maybe through some complicated but known function, like a neural network), and orange circle is the value we’re maximizing. In order to estimate the <span class="math inline">\(\mathcal{F}(\theta)\)</span> using such graph, you just take your <span class="math inline">\(\theta\)</span>s, compute <span class="math inline">\(x\)</span>’s distribution, take as many samples from it as you can get, compute <span class="math inline">\(f(x)\)</span> for each one, and then just average them.</p>
<p>How do we maximize it though? The workhorse of optimization in modern deep learning is the Stochastic Gradient Descent (or, in our case, Ascent), and if we want to apply it in our case, all we need to compute is an (preferably unbiased and low-variance) estimate of the gradient of the objective <span class="math inline">\(\nabla \mathcal{F}(\theta)\)</span>. This is seemingly easy for anyone familiar with basic calculus:</p>
<p><span class="math display">\[
\begin{align*}
\nabla_{\theta} \mathcal{F}(\theta)
&amp; = \nabla_{\theta} \mathbb{E}_{p(x \mid \theta)} f(x)
  = \nabla_{\theta} \int p(x \mid \theta) f(x) dx \\
&amp; = \int \nabla_{\theta} p(x \mid \theta) f(x) dx
  = \int \nabla_{\theta} \log p(x \mid \theta) f(x) p(x \mid \theta) dx \\
&amp; = \mathbb{E}_{p(x \mid \theta)} \nabla_{\theta} \log p(x \mid \theta) f(x) dx
\end{align*}
\]</span></p>
<p>There you have it! Just sample some <span class="math inline">\(x \sim p(x \mid \theta)\)</span>, calculate <span class="math inline">\(f(x)\)</span> using this sample, and then multiply the result by the gradient of log density – here’s your unbiased estimate of the true gradient. However, in practice people have observed that this estimator (called the <strong>score-function estimator</strong>, and also <strong>REINFORCE</strong> in reinforcement learning literature) has a large variance, making it impractical for high-dimensional <span class="math inline">\(x\)</span>.</p>
<p>And it kinda makes sense. Look at the estimator. It does not use gradient information of <span class="math inline">\(f\)</span>, so it does not have any guidance where to move <span class="math inline">\(p(x|\theta)\)</span> to make the expectation <span class="math inline">\(\mathcal{F}(\theta)\)</span> higher. Instead, it tries many random <span class="math inline">\(x\)</span>s, for each sample it takes the direction one should go to make this sample more probable, and weights these directions according to the magnitude of <span class="math inline">\(f(x)\)</span>. When averaged, this gives you true direction to maximize the objective, but it’s hard to randomly stumble upon good <span class="math inline">\(x\)</span> using just a few samples (especially early in training, or in high-dimensional spaces), hence high variance.</p>
<p>This manifests a necessity of either ways to improve the variance of such estimator, or different, more efficient approaches. In the following we will consider both.</p>
<h2 id="reparametrization-trick">Reparametrization trick</h2>
<p>Being perfectly aware of the aforementioned limitation, <a href="https://arxiv.org/abs/1312.6114">Kingma et. al</a> used a smart trick in their Variational Autoencoder paper. Basically, the idea is the following: if some random variables can be decomposed into combinations of other random variables, can we transform our stochastic computation graph such that we don’t need to backpropagate through randomness, and have stochasticity injected into the model as independent noise?</p>
<p>Turns out, we can. Namely, for any gaussian random variable <span class="math inline">\(x \sim \mathcal{N}(\mu, \sigma^2)\)</span> we can decompose it into affine transformation of some independent standard gaussian noise: <span class="math inline">\(x = \mu + \sigma \varepsilon\)</span> <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> where <span class="math inline">\(\varepsilon \sim \mathcal{N}(0, 1)\)</span>.</p>
The SCG then becomes
<div class="post-image">
<p><img src="../files/scg-gaussian-reparametrization.png" style="width: 400px" /></p>
</div>
<p>Here pink arrows denote the “flow” of backpropagation: notice that we do not encounter any sampling nodes along the way – hence we don’t need to use the high-variance score-function estimator. Let us look at the formulas.</p>
<p><span class="math display">\[
\nabla_\theta \mathbb{E}_{p(x|\theta)} f(x)
= \nabla_\theta \mathbb{E}_{p(\varepsilon)} f(\mu(\theta) + \sigma(\theta) \varepsilon)
= \mathbb{E}_{p(\varepsilon)} \nabla_\theta f(\mu(\theta) + \sigma(\theta) \varepsilon)
\]</span></p>
<p>Notice that this time we do use the gradient of <span class="math inline">\(f\)</span>! This is the crucial difference between this estimator, and the score-function one: in the later we were averaging random directions using their “scores”, whereas here we learn an affine transformation of independent noise such that transformed samples lie in an area that has large <span class="math inline">\(f(x)\)</span>. Gradient information of <span class="math inline">\(f\)</span> tells us where to move samples <span class="math inline">\(x\)</span>, and we do so by adjusting <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p>Okay, so it looks like a great method, why not use it everywhere? The problem is that even though you can always transform a uniformly distributed random variable into any other, it’s not always computationally easy <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. For some distributions (Dirichlet, for example <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>) we simply don’t know any effective transformations from parameter-free random variables.</p>
<h2 id="generalized-reparametrization-trick">Generalized reparametrization trick</h2>
<p>The reparametrization trick can be seen as a transformation <span class="math inline">\(\mathcal{T}(\varepsilon | \theta)\)</span> of some independent noise into a desired random variable. Conversely, if <span class="math inline">\(\mathcal{T}\)</span> is invertible, <span class="math inline">\(\mathcal{T}^{-1}(x | \theta)\)</span> is a “whitening” / “standardizing” transformation: it takes some random variable that depends on parameters <span class="math inline">\(\theta\)</span> and makes it parameter-independent.</p>
<p>What if we find a transformation that maybe does not whiten <span class="math inline">\(x\)</span> completely, but still significantly reduce its dependence on <span class="math inline">\(\theta\)</span>? This is the core idea of the <a href="https://arxiv.org/abs/1610.02287">The Generalized Reparameterization Gradient</a> paper. In that case <span class="math inline">\(\varepsilon\)</span> would still depend on <span class="math inline">\(\theta\)</span>, but hopefully only “weakly”.</p>
<p><span class="math display">\[
\begin{align*}
\nabla_\theta \mathbb{E}_{p(x|\theta)} f(x)
&amp;= \nabla_\theta \mathbb{E}_{p(\varepsilon|\theta)} f(\mathcal{T}(\varepsilon \mid \theta)) \\
&amp;= \underbrace{\mathbb{E}_{p(\varepsilon|\theta)} \nabla_\theta f(\mathcal{T}(\varepsilon \mid \theta))}_{g^\text{rep}}
+ \underbrace{\mathbb{E}_{p(\varepsilon|\theta)} \nabla_\theta \log p(\varepsilon|\theta) f(\mathcal{T}(\varepsilon \mid \theta))}_{g^\text{corr}}
\end{align*}
\]</span></p>
<p>Here <span class="math inline">\(g^\text{rep}\)</span> is our usual reparametrized gradient, and <span class="math inline">\(g^\text{corr}\)</span> is the score-function part of it. It’s easy to see that varying the transformation <span class="math inline">\(\mathcal{T}\)</span> allows you to interpolate between the fully reparametrized gradients and the fully score-function-based gradients. Indeed, if <span class="math inline">\(\mathcal{T}\)</span> whitens <span class="math inline">\(x\)</span> completely, then <span class="math inline">\(p(\varepsilon|\theta)\)</span> is independent of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\nabla_\theta \log p(\varepsilon|\theta) = 0\)</span>, leaving us with <span class="math inline">\(g^\text{rep}\)</span> only. If, however, <span class="math inline">\(\mathcal{T}\)</span> is an identity map, which does not do anything, then <span class="math inline">\(\nabla_\theta f(\mathcal{T}(\varepsilon \mid \theta)) = \nabla_\theta f(\varepsilon) = 0\)</span>, and we recover the score-function estimator.</p>
<p>This formula looks great, but it requires us to know the distribution of <span class="math inline">\(\mathcal{T}^{-1}(x \mid \theta)\)</span> to sample <span class="math inline">\(\varepsilon\)</span> from. It’s more convenient to reformulate the gradient in terms of samples from <span class="math inline">\(p(x|\theta)\)</span>, which we can do after some algebraic manipulations:</p>
<p><span class="math display">\[
\begin{align*}
g^\text{rep}
=&amp; \mathbb{E}_{p(x|\theta)} \nabla_x f(x) \nabla_\theta \mathcal{T}(\varepsilon \mid \theta)
\\
g^\text{corr}
=&amp; \mathbb{E}_{p(x|\theta)} \Bigl[\nabla_\theta \log p(x|\theta) + \nabla_x \log p(x|\theta) \nabla_\theta \mathcal{T}(\varepsilon \mid \theta) \\&amp; \quad\quad\quad\quad+ \nabla_\theta \log |\text{det} \nabla_\varepsilon \mathcal{T}(\varepsilon \mid \theta)|\Bigr] f(x)
\\
 &amp; \text{where } \varepsilon = \mathcal{T}^{-1}(x \mid \theta)
\end{align*}
\]</span></p>
<p>In this formulation we sample <span class="math inline">\(x\)</span> as usual, pass it through the “whitening” transformation <span class="math inline">\(\mathcal{T}^{-1}(x | \theta)\)</span> to obtain sample <span class="math inline">\(\varepsilon\)</span>, and substitute these variables into gradient constituents. One can also see everything but <span class="math inline">\(f(x) \nabla_\theta \log p(x \mid \theta)\)</span> as a <em>control variate</em> (we’ll talk about these later in the series) that uses <span class="math inline">\(f\)</span>’s gradient information and hence can be expected to be quite powerful.</p>
<p>The last question is which transformation to choose? The formulas authors propose to use usual standardizing transformation, i.e. to subtract the mean and divide by standard deviation. This choice is motivated by the following: a) it’s computationally convenient, recall that we need both <span class="math inline">\(\mathcal{T}\)</span> and <span class="math inline">\(\mathcal{T}^{-1}\)</span> <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>; b) it makes first two moments independent of <span class="math inline">\(\theta\)</span>, which is some sense makes resulting variable “weakly” dependent on it.</p>
<h3 id="rejection-sampling-perspective-5">Rejection sampling perspective <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></h3>
<p>Another interesting perspective on generalized reparametrization comes from the following thought: there are efficient samplers for many distributions, can we somehow backpropagate through the sampling process? This is what authors of the <a href="http://proceedings.mlr.press/v54/naesseth17a.html">Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms</a> paper decided to find out.</p>
<p>You want to sample some distribution <span class="math inline">\(p(x|\theta)\)</span>, but can’t compute and invert its CDF, what to do then? You can use <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> procedure. Basically, you take some proposal distribution <span class="math inline">\(r(x \mid \theta)\)</span> that is easy to sample from, find a scaling factor <span class="math inline">\(M_\theta\)</span> such that scaled proposal is uniformly higher than the target density for all <span class="math inline">\(x\)</span>: <span class="math inline">\(M_\theta r(x|\theta) \ge p(x|\theta) \forall x\)</span>. Then you generate points randomly under the scaled <span class="math inline">\(M_\theta r(x|\theta)\)</span> curve, and keep only those that are also below the <span class="math inline">\(p(x|\theta)\)</span> curve:</p>
<ol style="list-style-type: decimal">
<li>Generate <span class="math inline">\(x \sim r(x|\theta)\)</span>.</li>
<li>Generate <span class="math inline">\(u \sim U[0, M_\theta r(x|\theta)]\)</span>.</li>
<li>If <span class="math inline">\(u &gt; p(x|\theta)\)</span>, repeat from step 1, else return <span class="math inline">\(x\)</span>.</li>
</ol>
<p>Moreover, at step 1 we can use some transformation <span class="math inline">\(\mathcal{T}(\varepsilon | \theta)\)</span> of the sample <span class="math inline">\(\varepsilon \sim r(\varepsilon)\)</span> (provided the scaled density of transformed variable is uniformly higher). This is how <code>numpy</code> generates Gamma variables: if samples <span class="math inline">\(\varepsilon\)</span> from standard Gaussian, transforms the sample through some function <span class="math inline">\(x = \mathcal{T}(\varepsilon|\theta)\)</span>, and then accepts it with probability <span class="math inline">\(a(x|\theta)\)</span> <a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>.</p>
<p>Let’s find the density of <span class="math inline">\(\varepsilon\)</span>s that lead to acceptance of corresponding <span class="math inline">\(x\)</span>s. Some calculations (provided in supplementary) show that</p>
<p><span class="math display">\[
p(\varepsilon|\theta) = M_\theta r(\varepsilon) a(\mathcal{T}(\varepsilon|\theta)|\theta)
\]</span></p>
<p>Note that this density is easy to calculate, and if we reparametrize generated samples <span class="math inline">\(\varepsilon\)</span>, we’d get samples <span class="math inline">\(x\)</span> we’re looking for <span class="math inline">\(x = \mathcal{T}(\varepsilon|\theta)\)</span>. Hence the objective becomes</p>
<p><span class="math display">\[
\mathcal{F}(\theta) = \mathbb{E}_{p(\varepsilon|\theta)} f(\mathcal{T}(\varepsilon|\theta))
\]</span></p>
<p>Differentiating it w.r.t. <span class="math inline">\(\theta\)</span> gives <span class="math display">\[
\nabla_\theta \mathcal{F}(\theta)
= \mathbb{E}_{p(\varepsilon|\theta)} \nabla_\theta f(\mathcal{T}(\varepsilon|\theta))
+ \mathbb{E}_{p(\varepsilon|\theta)} f(\mathcal{T}(\varepsilon|\theta)) \nabla_\theta \log p(\varepsilon|\theta)
\]</span></p>
<p>Now compare these addends to the <span class="math inline">\(g^\text{rep}\)</span> and <span class="math inline">\(g^\text{corr}\)</span> from the previous section. You can see that they’re <em>exactly</em> the same!</p>
<p>In the previous section we choose the transformation <span class="math inline">\(\mathcal{T}^{-1}\)</span> such that it tries to remove at least some dependency on <span class="math inline">\(\theta\)</span> from samples <span class="math inline">\(x\)</span>. This section allows us to view the same method from the other end: if you have some independent noise <span class="math inline">\(\varepsilon\)</span> and a transformation <span class="math inline">\(\mathcal{T}\)</span> that makes the samples look like samples from the target density <span class="math inline">\(p(x|\theta)\)</span>, then you can add some rejection sampling on top to compensate for the mismatch, and still enjoy the lower variance of gradient estimate.</p>
<h2 id="a-very-simple-example">A (very) simple example</h2>
<p>Let’s see how much variance reduction the reparametrization trick actually gets us in a very simple problem. Namely, let’s try to minimize expected square of a Gaussian random variable <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> (shifted by some positive constant <span class="math inline">\(c\)</span>, we will see later how it comes into play):</p>
<p><span class="math display">\[
\mathcal{F}(\mu, \sigma) = \mathbb{E}_{x \sim \mathcal{N}(\mu, \sigma^2)} [x^2 + c] \to \min_{\mu, \sigma}
\]</span></p>
<p>First, reparametrized objective is</p>
<p><span class="math display">\[
\mathcal{F}^\text{rep}(\mu, \sigma) = \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)} (\mu + \sigma \varepsilon)^2
\]</span></p>
<p>And its stochastic gradients are <span class="math display">\[
\hat \nabla_\mu \mathcal{F}^\text{rep}(\mu, \sigma) = 2 (\mu + \sigma \varepsilon) \\
\hat \nabla_\sigma \mathcal{F}^\text{rep}(\mu, \sigma) = 2 \varepsilon (\mu + \sigma \varepsilon)
\]</span></p>
<p>The score-function-based gradients are the following:</p>
<p><span class="math display">\[
\hat \nabla_\mu \mathcal{F}^\text{SF}(\mu, \sigma) = \frac{\varepsilon}{\sigma} \left((\mu + \sigma \varepsilon)^2 + c\right) \\
\hat \nabla_\sigma \mathcal{F}^\text{SF}(\mu, \sigma) = \frac{\varepsilon^2 - 1}{\sigma} \left((\mu + \sigma \varepsilon)^2 + c\right)
\]</span></p>
<p>Both estimators are unbiased, but what are the variances of these estimators? WolframAlpha suggests</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{D}\left[\hat \nabla_\mu \mathcal{F}^\text{SF}(\mu, \sigma)\right] &amp;= \frac{(\mu^2 + c)^2}{\sigma^2} + 15 \sigma^2 + 14 \mu^2 + 6 c,
\\
\mathbb{D}\left[\hat \nabla_\mu \mathcal{F}^\text{rep}(\mu, \sigma)\right] &amp;= 4 \sigma^2
\\
\mathbb{D}\left[\hat \nabla_\sigma \mathcal{F}^\text{SF}(\mu, \sigma)\right] &amp;= \frac{2 (c + \mu^2)^2}{\sigma^{2}} + 60 \mu^{2} + 74 \sigma^{2} + 20 c,
\\
\mathbb{D}\left[\hat \nabla_\sigma \mathcal{F}^\text{rep}(\mu, \sigma)\right] &amp;= 4 \mu^2 + 8 \sigma^2
\end{align*}
\]</span></p>
<p>You can see that not only the score-function-based gradient always has a higher variance, its variance actually explodes as we approach <span class="math inline">\(\mu = 0, \sigma = 0\)</span> (unless <span class="math inline">\(c = 0\)</span> and <span class="math inline">\(\mu\)</span> is small enough to counter <span class="math inline">\(\sigma\)</span>)! This is due to the fact that as your variance shrinks, points somewhat far away from the mean get very tiny probabilities, hence score-function-based gradients thinks it should try very hard to make them more probable.</p>
<p>You might be wodering, how would generalized reparametrization work? If we consider <span class="math inline">\(\mathcal{T}^{-1}(x|\mu,\sigma) = x - \mu\)</span> transformation (it “whitens” first moment only), then we obtain the following gradient estimates:</p>
<p><span class="math display">\[
\hat \nabla_\mu \mathcal{F}^\text{g-rep}(\mu, \sigma) = 2 (\mu + \varepsilon) \\
\hat \nabla_\sigma \mathcal{F}^\text{g-rep}(\mu, \sigma) = \frac{\varepsilon^2 - \sigma^2}{\sigma^3} (\mu + \varepsilon)^2
\]</span></p>
<p>This is the reparametrized gradient w.r.t. <span class="math inline">\(\mu\)</span> and score-function gradient w.r.t. <span class="math inline">\(\sigma\)</span> (notice that <span class="math inline">\(\varepsilon \sim \mathcal{N}(0, \sigma^2)\)</span> in this case). I don’t think this is an interesting scenario, so instead we’ll consider a weird-looking second-moment-whitening transformation <span class="math inline">\(\mathcal{T}^{-1}(x|\mu,\sigma) = \frac{x - \mu}{\sigma} + \mu\)</span> with <span class="math inline">\(\mathcal{T}(\varepsilon|\mu,\sigma) = \sigma (\epsilon - \mu) + \mu\)</span>. The gradients for this transformation are:</p>
<p><span class="math display">\[
\begin{align*}
\hat \nabla_\mu \mathcal{F}^\text{g-rep}(\mu, \sigma) &amp;=
\left(c + \left(\mu + \sigma \left(\epsilon - \mu\right)\right)^{2}\right) \left(\epsilon - \mu\right) - 2 \left(\mu + \sigma \left(\epsilon - \mu\right)\right) \left(\sigma - 1\right)
\\
\hat \nabla_\sigma \mathcal{F}^\text{g-rep}(\mu, \sigma) &amp;=
2 \left(\epsilon - \mu\right) \left(\mu + \sigma \left(\epsilon - \mu\right)\right)
\end{align*}
\]</span></p>
<p>You can already see that the magnitude of the gradients does not explode when the variance <span class="math inline">\(\sigma\)</span> goes to zero. Let’s check the variances:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{D}\left[\hat \nabla_\mu \mathcal{F}^\text{g-rep}(\mu, \sigma)\right] &amp;=
(\mu^2 + c)^{2} + 2 c \sigma^{2} + 4 c \sigma + 10 \mu^{2} \sigma^{2} + 4 \mu^{2} \sigma + 7 \sigma^{4} + 4 \sigma^{3} + 4 \sigma^{2}
\\
\mathbb{D}\left[\hat \nabla_\sigma \mathcal{F}^\text{g-rep}(\mu, \sigma)\right] &amp;=
4 \mu^{2} + 8 \sigma^{2}
\end{align*}
\]</span></p>
<p>First, we see that the variance of gradient w.r.t. <span class="math inline">\(\sigma\)</span> has become identical to the variance of the reparametrized case. Second, we can confirm that the variance does not explode as we approach the optimum.</p>
<div class="post-image">
<p><img src="../files/scg-example.png" /> Gen Rep 1 is a generalized reparametrization with only 1st moment whitened out,<br /> Gen Rep 2 – with only the second one</p>
</div>
<p>The simulation plots clearly show that score-function-based gradients and the first generalized reparametrization fail to converge, which is in line with our variance analysis. The second generalized reparametrization, however, performs just as good, as the full reparametrization, even though it does have higher variance.</p>
<p>All the code I wrote while working on this post can be found <a href="https://gist.github.com/artsobolev/fec7c052d712889ef69656825634c4d4">here</a>. Though it’s quite messy, I warned you.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We have discussed tricks that make Stochastic Variational Inference with continuous latent variables computationally feasible. However, quite often we’re interested in models with discrete latent variables – for example, we might be interested in a model that dynamically chooses one computation path or another, essentially controlling how much computation time to spend on a given sample. Or, train a GAN for textual data – we need a way to backpropagate through discriminator’s inputs.</p>
<p>We’ll talk about such methods in the next post.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Equality here means both sides have the same distribution.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>We know that for <span class="math inline">\(X \sim p(x)\)</span> with c.d.f. <span class="math inline">\(F(x)\)</span> we have <span class="math inline">\(F(X) \sim U[0, 1]\)</span>, hence <span class="math inline">\(X = F^{-1}(u)\)</span> for standard uniform <span class="math inline">\(u \sim U[0, 1]\)</span>, so there always exist a (smooth, if <span class="math inline">\(x\)</span> is continuous) transformation from standard uniform noise to any other distribution. However, computing CDF function often requires expensive integration, which is quite often infeasible.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Original VAE paper lists Dirichlet distribution among ones that have effective reparametrizations, however that’s actually not the case, as you still need to generate parametrized Gamma variables.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Technically, you could derive the density <span class="math inline">\(p(\varepsilon|\theta)\)</span> and to sample from it – this way you’d not need the inverse <span class="math inline">\(\mathcal{T}^{-1}\)</span>. However, it’s not easy in general.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>This section is largely based on the <a href="https://casmls.github.io/general/2017/04/25/rsvi.html">Reparameterization Gradients through Rejection Sampling Algorithms</a> blogpost.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Normally that’d be just <span class="math inline">\(a(x|\theta) = \tfrac{p(x|\theta)}{M_\theta r(x|\theta)}\)</span>, however, if we don’t have <span class="math inline">\(r(x|\theta)\)</span> readily available, we can express the acceptance probability in terms of <span class="math inline">\(\varepsilon\)</span>: <span class="math display">\[a(\varepsilon|\theta) = \tfrac{p(\mathcal{T}(\varepsilon|\theta)|\theta) |\text{det} \nabla_\varepsilon \mathcal{T}(\varepsilon|\theta)|}{M_\theta r(\varepsilon)}\]</span><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>One might argue that our approach is flawed, as the optimal distribution is <span class="math inline">\(\mathcal{N}(0, 0)\)</span> which is not a valid distribution. However, here we’re just interested in the gradient dynamics as we approach this optimum.<a href="#fnref7">↩</a></p></li>
</ol>
</div>
</section>

<div class="tags-pane">Tagged as <a href="../tags/machine%20learning.html">machine learning</a>, <a href="../tags/deep%20learning.html">deep learning</a>, <a href="../tags/variational%20inference.html">variational inference</a>, <a href="../tags/stochastic%20computation%20graphs%20series.html">stochastic computation graphs series</a></div>
</article>

<section class="post-comments">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'barmaley-exe'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

    </section>
    <footer>
        <a href="http://jaspervdj.be/hakyll/index.html">Generated with Hakyll</a>
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
