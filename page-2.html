<!DOCTYPE html>
<html lang="en">
<head>
  <title>page-2 - B.log</title>
  <meta charset="utf-8" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta name="twitter:title" content="page-2" />

  <meta name="twitter:description" content />


  <link rel="shortcut icon" href="./favicon.ico" />

  <link rel="stylesheet" type="text/css" href="./css/default.css" />
  <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Register.StartupHook("TeX Jax Ready", function () {
      MathJax.InputJax.TeX.Definitions.Add({
              macros: {
                        E: ["Macro", "\\mathop{\\mathbb{E}}"]
                      }
            });
});
  </script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="./">B.log</a></h1>
        <h2>Random notes mostly on Machine Learning</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="./">Home</a>
          <a href="./pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
        <article>
	<header>
		<h3><a href="./posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">Neural Variational Inference: Variational Autoencoders and Helmholtz machines</a></h3>
		<time>July 11, 2016</time>
	</header>

	<section>
		<p>So far we had a little of “neural” in our VI methods. Now it’s time to fix it, as we’re going to consider <a href="https://arxiv.org/abs/1312.6114">Variational Autoencoders</a> (VAE), a paper by D. Kingma and M. Welling, which made a lot of buzz in ML community. It has 2 main contributions: a new approach (AEVB) to large-scale inference in non-conjugate models with continuous latent variables, and a probabilistic model of autoencoders as an example of this approach. We then discuss connections to <a href="https://en.wikipedia.org/wiki/Helmholtz_machine">Helmholtz machines</a> — a predecessor of VAEs.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-07-05-neural-variational-inference-blackbox.html">Neural Variational Inference: Blackbox Mode</a></h3>
		<time>July  5, 2016</time>
	</header>

	<section>
		<p>In the <a href="./posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html">previous post</a> we covered Stochastic VI: an efficient and scalable variational inference method for exponential family models. However, there’re many more distributions than those belonging to the exponential family. Inference in these cases requires significant amount of model analysis. In this post we consider <a href="https://arxiv.org/abs/1401.0118">Black Box Variational Inference</a> by Ranganath et al. This work just as the previous one comes from <a href="http://www.cs.columbia.edu/~blei/">David Blei</a> lab — one of the leading researchers in VI. And, just for the dessert, we’ll touch upon another paper, which will finally introduce some neural networks in VI.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html">Neural Variational Inference: Scaling Up</a></h3>
		<time>July  4, 2016</time>
	</header>

	<section>
		<p>In the <a href="./posts/2016-07-01-neural-variational-inference-classical-theory.html">previous post</a> I covered well-established classical theory developed in early 2000-s. Since then technology has made huge progress: now we have much more data, and a great need to process it and process it fast. In big data era we have huge datasets, and can not afford too many full passes over it, which might render classical VI methods impractical. Recently M. Hoffman et al. dissected classical Mean-Field VI to introduce stochasticity right into its heart, which resulted in <a href="https://arxiv.org/abs/1206.7051">Stochastic Variational Inference</a>.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-07-01-neural-variational-inference-classical-theory.html">Neural Variational Inference: Classical Theory</a></h3>
		<time>July  1, 2016</time>
	</header>

	<section>
		<p>As a member of <a href="http://bayesgroup.ru/">Bayesian methods research group</a> I’m heavily interested in Bayesian approach to machine learning. One of the strengths of this approach is ability to work with hidden (unobserved) variables which are interpretable. This power however comes at a cost of generally intractable exact inference, which limits the scope of solvable problems.</p>
<p>Another topic which gained lots of momentum in Machine Learning recently is Deep Learning, of course. With Deep Learning we can now build big and complex models that outperform most hand-engineered approaches given lots of data and computational power. The fact that Deep Learning needs a considerable amount of data also requires these methods to be scalable — a really nice property for any algorithm to have, especially in a Big Data epoch.</p>
<p>Given how appealing both topics are it’s not a surprise there’s been some work to marry these two recently. In this <a href="./tags/modern%20variational%20inference%20series.html">series</a> of blogsposts I’d like to summarize recent advances, particularly in variational inference. This is not meant to be an introductory discussion as prior familiarity with classical topics (Latent variable models, <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference, Mean-field approximation</a>) is required, though I’ll introduce these ideas anyway just to remind it and setup the notation.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2014-08-01-gnu-parallel.html">Exploiting Multiple Machines for Embarrassingly Parallel Applications</a></h3>
		<time>August  1, 2014</time>
	</header>

	<section>
		<p>During work on my machine learning project I was needed to perform some quite computation-heavy calculations several times — each time with a bit different inputs. These calculations were CPU and memory bound, so just spawning them all at once would just slow down overall running time because of increased amount of context switches. Yet running 4 (=number of cores in my CPU) of them at a time (actually, 3 since other applications need CPU, too) should speed it up.</p>
<p>Fortunately, I have an old laptop with 2 cores as well as an access to somewhat more modern machine with 4 cores. That results in 10 cores spread across 3 machines (all of`em have some version of GNU Linux installed). The question was how to exploit such a treasury.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2014-05-01-on-sorting-complexity.html">On Sorting Complexity</a></h3>
		<time>May  1, 2014</time>
	</header>

	<section>
		<p>It’s well known that lower bound for sorting problem (in general case) is <span class="math inline">\(\Omega(n \log n)\)</span>. The proof I was taught is somewhat involved and is based on paths in “decision” trees. Recently I’ve discovered an information-theoretic approach (or reformulation) to that proof.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2013-05-23-js-namespaced-methods.html">Namespaced Methods in JavaScript</a></h3>
		<time>May 23, 2013</time>
	</header>

	<section>
		<p>Once upon a time I was asked (well, actually <a href="http://habrahabr.ru/qa/7130/" title="Javascript: String.prototype.namespace.method и this / Q&A / Хабрахабр">a question</a> wasn’t for me only, but for whole habrahabr’s community) is it possible to implement namespaced methods in JavaScript for built-in types like:</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="dv">5</span>..<span class="va">rubish</span>.<span class="at">times</span>(<span class="kw">function</span>() <span class="op">{</span> <span class="co">// this function will be called 5 times</span>
  <span class="va">console</span>.<span class="at">log</span>(<span class="st">&quot;Hi there!&quot;</span>)<span class="op">;</span>
<span class="op">}</span>)<span class="op">;</span>

<span class="st">&quot;some string&quot;</span>.<span class="va">hask</span>.<span class="at">map</span>(<span class="kw">function</span>(c) <span class="op">{</span> <span class="cf">return</span> <span class="va">c</span>.<span class="va">hask</span>.<span class="at">code</span>()<span class="op">;</span> <span class="op">}</span>)<span class="op">;</span>
<span class="co">// equivalent to</span>
<span class="st">&quot;some string&quot;</span>.<span class="at">split</span>(<span class="st">''</span>).<span class="at">map</span>(<span class="kw">function</span>(c) <span class="op">{</span> <span class="cf">return</span> <span class="va">c</span>.<span class="at">charCodeAt</span>()<span class="op">;</span> <span class="op">}</span>)<span class="op">;</span>

<span class="st">&quot;another string&quot;</span>.<span class="va">algo</span>.<span class="at">lcp</span>(<span class="st">&quot;annotation&quot;</span>)<span class="op">;</span> 
<span class="co">// returns longest common prefix of two strings</span></code></pre></div>
<p>As you can see at the link, it’s possible using ECMAScript 5 features. And that’s how: 
	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2013-03-30-crazy-expression-parsing.html">Crazy Expression Parsing</a></h3>
		<time>March 30, 2013</time>
	</header>

	<section>
		<p>Suppose we have an expression like <code>(5+5 * (x^x-5 | y &amp;&amp; 3))</code> and we’d like to get some computer-understandable representation of that expression, like:</p>
<p><code>ADD Token[5] (MUL Token[5] (AND (BIT_OR (XOR Token[x] (SUB Token[x] Token[5])) Token[y]) Token[3])</code></p>
<p>In case if you don’t know how to do that or are looking for the solutin right now, you should know that I’m not going to present a correct solution. This post is just a joke. You should use either a <a href="http://en.wikipedia.org/wiki/Shunting-yard_algorithm" title="Shunting-yard algorithm — Wikipedia">Shunting-yard algorithm</a> or a <a href="http://en.wikipedia.org/wiki/Recursive_descent_parser">recursive descent parser</a>.</p>
<p>So if you’re ready for madness… Let’s go! 
	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2013-03-29-cpp-11-memoization.html">Memoization Using C++11</a></h3>
		<time>March 29, 2013</time>
	</header>

	<section>
		<p>Recently I’ve read an article <a href="http://john-ahlgren.blogspot.ru/2013/03/efficient-memoization-using-partial.html" title="John Ahlgren: Efficient Memoization using Partial Function Application">Efficient Memoization using Partial Function Application</a>. Author explains function memoization using partial application. When I was reading the article, I thought “Hmmm, can I come up with a more general solution?” And as suggested in comments, one can use variadic templates to achieve it. So here is my version.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2013-02-10-std-vector-growth.html">Resizing Policy of std::vector</a></h3>
		<time>February 10, 2013</time>
	</header>

	<section>
		Sometime ago when Facebook opensourced their <a title="Folly is an open-source C++ library developed and used at Facebook" href="https://github.com/facebook/folly">Folly library</a> I was reading their docs and found <a title="folly/FBvector.h documentation" href="https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md">something interesting</a>. In section “Memory Handling” they state
<blockquote>
In fact it can be mathematically proven that a growth factor of 2 is rigorously the worst possible because it never allows the vector to reuse any of its previously-allocated memory
</blockquote>
<p>I haven’t got it first time. Recently I recalled that article and decided to deal with it. So after reading and googling for a while I finally understood the idea, so I’d like to say a few words about it.</p>

	</section>
</article>

<hr />


<div class="paginator">
<div class="paginator-newer">

<a href="./index.html">&larr; Newer Entries</a>
&nbsp;
</div>
<div class="paginator-older">
&nbsp;
</div>
</div>

    </section>
    <footer>
        <a href="http://jaspervdj.be/hakyll/index.html">Generated with Hakyll</a>
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
