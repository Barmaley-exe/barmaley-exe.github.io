<!DOCTYPE html>
<html lang="en">
<head>
  <title>index - B.log</title>
  <meta charset="utf-8" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@art_sobolev" />
  <meta name="twitter:title" content="index" />

  <meta name="twitter:description" content />


  <link rel="shortcut icon" href="./favicon.ico" />

  <link rel="stylesheet" type="text/css" href="./css/default.css" />
  <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
    <header>
      <hgroup>
        <h1><a href="./">B.log</a></h1>
        <h2>Random notes on Computer Science, Mathematics and Software Engineering</h2>
      </hgroup>
    </header>
    <nav>
        <menu>
          <a href="./">Home</a>
          <a href="./pages/about.html">About me</a>
          <a href="http://feeds.feedburner.com/barmaley-exe-blog-feed">RSS feed</a>
        </menu>
    </nav>
    <section>
        <article>
	<header>
		<h3><a href="./posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html">Neural Samplers and Hierarchical Variational Inference</a></h3>
		<time>April 26, 2019</time>
	</header>

	<section>
		<p>This post sets background for the upcoming post on my work on more efficient use of neural samplers for Variational Inference.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-11-12-stochastic-computation-graphs-fixing-reinforce.html">Stochastic Computation Graphs: Fixing REINFORCE</a></h3>
		<time>November 12, 2017</time>
	</header>

	<section>
		<p>This is the final post of the <a href="./tags/stochastic%20computation%20graphs%20series.html">stochastic computation graphs series</a>. Last time we discussed models with <a href="./posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html">discrete relaxations of stochastic nodes</a>, which allowed us to employ the power of reparametrization.</p>
<p>These methods, however, posses one flaw: they consider different models, thus introducing inherent bias – your test time discrete model will be doing something different from what your training time model did. Therefore in this post we’ll get back to the REINFORCE aka Score Function estimator, and see if we can fix its problems.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html">Stochastic Computation Graphs: Discrete Relaxations</a></h3>
		<time>October 28, 2017</time>
	</header>

	<section>
		<p>This is the second post of the <a href="./tags/stochastic%20computation%20graphs%20series.html">stochastic computation graphs series</a>. Last time we discussed models with <a href="./posts/2017-09-10-stochastic-computation-graphs-continuous-case.html">continuous stochastic nodes</a>, for which there are powerful reparametrization technics.</p>
<p>Unfortunately, these methods don’t work for discrete random variables. Moreover, it looks like there’s no way to backpropagate through discrete stochastic nodes, as there’s no infinitesimal change of random values when you infinitesimally perturb their parameters.</p>
<p>In this post I’ll talk about continuous relaxations of discrete random variables.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-09-10-stochastic-computation-graphs-continuous-case.html">Stochastic Computation Graphs: Continuous Case</a></h3>
		<time>September 10, 2017</time>
	</header>

	<section>
		<p>Last year I covered <a href="./tags/modern%20variational%20inference%20series.html">some modern Variational Inference theory</a>. These methods are often used in conjunction with Deep Neural Networks to form deep generative models (VAE, for example) or to enrich deterministic models with stochastic control, which leads to better exploration. Or you might be interested in amortized inference.</p>
<p>All these cases turn your computation graph into a stochastic one – previously deterministic nodes now become random. And it’s not obvious how to do backpropagation through these nodes. In <a href="./tags/stochastic%20computation%20graphs%20series.html">this series</a> I’d like to outline possible approaches. This time we’re going to see why general approach works poorly, and see what we can do in a continuous case.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-08-14-icml-2017.html">ICML 2017 Summaries</a></h3>
		<time>August 14, 2017</time>
	</header>

	<section>
		<p>Just like with <a href="./posts/2016-12-31-nips-2016-summaries.html">NIPS last year</a>, here’s a list of ICML’17 summaries (updated as I stumble upon new ones)</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-07-23-no-free-lunch-theorem.html">On No Free Lunch Theorem and some other impossibility results</a></h3>
		<time>July 23, 2017</time>
	</header>

	<section>
		<p>The more I talk to people online, the more I hear about the famous No Free Lunch Theorem (NFL theorem). Unfortunately, quite often people don’t really understand what the theorem is about, and what its implications are. In this post I’d like to share my view on the NFL theorem, and some other impossibility results.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2017-01-29-matrix-and-vector-calculus-via-differentials.html">Matrix and Vector Calculus via Differentials</a></h3>
		<time>January 29, 2017</time>
	</header>

	<section>
		<p>Many tasks of machine learning can be posed as optimization problems. One comes up with a parametric model, defines a loss function, and then minimizes it in order to learn optimal parameters. One very powerful tool of optimization theory is the use of smooth (differentiable) functions: those that can be locally approximated with a linear functions. We all surely know how to differentiate a function, but often it’s more convenient to perform all the derivations in matrix form, since many computational packages like numpy or matlab are optimized for vectorized expressions.</p>
<p>In this post I want to outline the general idea of how one can calculate derivatives in vector and matrix spaces (but the idea is general enough to be applied to other algebraic structures).</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-12-31-nips-2016-summaries.html">NIPS 2016 Summaries</a></h3>
		<time>December 31, 2016</time>
	</header>

	<section>
		<p>I did not attend this year’s NIPS, but I’ve gathered many summaries published online by those who did attend the conference.</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html">Neural Variational Inference: Importance Weighted Autoencoders</a></h3>
		<time>July 14, 2016</time>
	</header>

	<section>
		<p>Previously we covered <a href="./posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">Variational Autoencoders</a> (VAE) — popular inference tool based on neural networks. In this post we’ll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, <a href="https://arxiv.org/abs/1509.00519">Importance Weighted Autoencoders</a> (IWAE). The crucial contribution of this work is introduction of a new lower-bound on the marginal log-likelihood <span class="math inline">\(\log p(x)\)</span> which generalizes ELBO, but also allows one to use less accurate approximate posteriors <span class="math inline">\(q(z \mid x, \Lambda)\)</span>.</p>
<p>On a dessert we’ll discuss another paper, <a href="https://arxiv.org/abs/1602.06725">Variational inference for Monte Carlo objectives</a> by A. Mnih and D. Rezende which aims to broaden the applicability of this approach to models where reparametrization trick can not be used (e.g. for discrete variables).</p>

	</section>
</article>

<hr />
<article>
	<header>
		<h3><a href="./posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">Neural Variational Inference: Variational Autoencoders and Helmholtz machines</a></h3>
		<time>July 11, 2016</time>
	</header>

	<section>
		<p>So far we had a little of “neural” in our VI methods. Now it’s time to fix it, as we’re going to consider <a href="https://arxiv.org/abs/1312.6114">Variational Autoencoders</a> (VAE), a paper by D. Kingma and M. Welling, which made a lot of buzz in ML community. It has 2 main contributions: a new approach (AEVB) to large-scale inference in non-conjugate models with continuous latent variables, and a probabilistic model of autoencoders as an example of this approach. We then discuss connections to <a href="https://en.wikipedia.org/wiki/Helmholtz_machine">Helmholtz machines</a> — a predecessor of VAEs.</p>

	</section>
</article>

<hr />


<div class="paginator">
<div class="paginator-newer">
&nbsp;
</div>
<div class="paginator-older">

<a href="./page-2.html">Older Entries &rarr;</a>
&nbsp;
</div>
</div>

    </section>
    <footer>
        <a href="http://jaspervdj.be/hakyll/index.html">Generated with Hakyll</a>
    </footer>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-38530232-1']);
_gaq.push(['_trackPageview']);
(function() {
 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
